%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template: Article
%
% Por: Abrantes Araújo Silva Filho
%      abrantesasf@gmail.com
%
% Citação: Se você gostou deste template, por favor ajude a divulgá-lo mantendo
%          o link para meu repositório GitHub em:
%          https://github.com/abrantesasf/LaTeX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configura o tipo de documento, papel, tamanho da fonte e informações básicas
%%% para as proriedades do PDF/DVIPS e outras propriedades do documento
\RequirePackage{ifpdf}
\ifpdf
  % Classe, língua e tamanho da fonte padrão. Outras opções a considerar:
  %   draft
  %   onecolumn (padrão) ou twocolumn (OU usar o package multicol)
  %   fleqn com ou sem leqno (alinhamento à esquerda das fórmulas e dos números)
  %   oneside (padrão para article ou report) ou twoside (padrão para book)
  \documentclass[pdftex, brazil, 12pt, twoside]{article}
\else
  % Classe, língua e tamanho da fonte padrão. Outras opções a considerar:
  %   draft
  %   onecolumn (padrão) ou twocolumn (OU usar o package multicol)
  %   fleqn com ou sem leqno (alinhamento à esquerda das fórmulas e dos números)
  %   oneside (padrão para article ou report) ou twoside (padrão para book)
  \documentclass[brazil, 12pt]{article}
\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Carrega pacotes iniciais necessários para estrutura de controle e para a
%%% criação e o parse de novos comandos
\usepackage{ifthen}
\usepackage{xparse}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configuração do tamanho da página, margens, espaçamento entrelinhas e, se
%%% necessário, ativa a indentação dos primeiros parágrafos.
\ifpdf
  \usepackage[pdftex]{geometry}
\else
  \usepackage[dvips]{geometry}
\fi
\geometry{a4paper, left=2.6cm, right=4.0cm, top=3.0cm, bottom=3.4cm}

\usepackage{setspace}
  \singlespacing
  %\onehalfspacing
  %\doublespacing


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configurações de cabeçalho e rodapé:
\usepackage{fancyhdr}
\setlength{\headheight}{1cm}
\setlength{\footskip}{1.5cm}
\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.0pt}
\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{%
  \markboth{\uppercase{#1}}{}}
\renewcommand{\subsectionmark}[1]{%
  \markright{\uppercase{\thesubsection \hspace{0.1cm} #1}}{}}
\fancyhead{}
\fancyfoot{}
\newcommand{\diminuifonte}{%
    \fontsize{9pt}{9}\selectfont
}
\newcommand{\aumentafonte}{%
    \fontsize{12}{12}\selectfont
}
% Configura cabeçalho e rodapé para documentos TWOSIDE
\fancyhead[EL]{\textbf{\thepage}}
\fancyhead[EC]{}
\fancyhead[ER]{\diminuifonte \textbf{\leftmark}}
\fancyhead[OR]{\textbf{\thepage}}
\fancyhead[OC]{}
\fancyhead[OL]{\diminuifonte \textbf{\rightmark}}
\fancyfoot[EL,EC,ER,OR,OC,OL]{}
% Configura cabeçalho e rodapé para documentos ONESIDE
%\lhead{ \fancyplain{}{sup esquerdo} }
%\chead{ \fancyplain{}{sup centro} }
%\rhead{ \fancyplain{}{\thesection} }
%\lfoot{ \fancyplain{}{inf esquerdo} }
%\cfoot{ \fancyplain{}{inf centro} }
%\rfoot{ \fancyplain{}{\thepage} }




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configurações de encoding, lingua e fontes:
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}

% Altera a fonte padrão do documento (nem todas funcionam em modo math):
%   phv = Helvetica
%   ptm = Times
%   ppl = Palatino
%   pbk = bookman
%   pag = AdobeAvantGarde
%   pnc = Adobe NewCenturySchoolBook
\renewcommand{\familydefault}{ppl}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Carrega pacotes para referências cruzadas, citações dentro do documento,
%%% links para internet e outros.Configura algumas opções.
%%% Não altere a ordem de carregamento dos packages.
\usepackage{varioref}
\ifpdf
  \usepackage[pdftex]{hyperref}
    \hypersetup{
      % Informações variáveis em cada documento (MUDE AQUI!):
      pdftitle={Probability --- The Science of Uncertainty and Data},
      pdfauthor={MITx on EdX},
      pdfsubject={MITx 6.431x},
      pdfkeywords={probability, statistics, data science},
      pdfinfo={
        CreationDate={}, % Ex.: D:AAAAMMDDHH24MISS
        ModDate={}       % Ex.: D:AAAAMMDDHH24MISS
      },
      % Coisas que você não deve alterar se não souber o que está fazendo:
      unicode=true,
      pdflang={pt-BR},
      bookmarksopen=true,
      bookmarksnumbered=true,
      bookmarksopenlevel=5,
      pdfdisplaydoctitle=true,
      pdfpagemode=UseOutlines,
      pdfstartview=FitH,
      pdfcreator={LaTeX with hyperref package},
      pdfproducer={pdfTeX},
      pdfnewwindow=true,
      colorlinks=true,
      citecolor=green,
      linkcolor=red,
      filecolor=cyan,
      urlcolor=blue
    }
\else
  \usepackage{hyperref}
\fi
\usepackage{cleveref}
\usepackage{url}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Carrega bibliotecas de símbolos (matemáticos, físicos, etc.), fontes
%%% adicionais, e configura algumas opções
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{siunitx}
  \sisetup{group-separator = {.}}
  \sisetup{group-digits = {false}}
  \sisetup{output-decimal-marker = {,}}
\usepackage{bm}
\usepackage{cancel}
% Altera separador decimal via comando, se necessário (prefira o siunitx):
%\mathchardef\period=\mathcode`.
%\DeclareMathSymbol{.}{\mathord}{letters}{"3B}
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Carrega packages relacionados à computação
\usepackage{algorithm2e}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}
  \lstset{literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
    {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
    {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
    {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
    {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  }
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Ativa suporte extendido a cores
\usepackage[svgnames]{xcolor} % Opções de cores: usenames (16), dvipsnames (64),
                              % svgnames (150) e x11names (300).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Suporte à importação de gráficos externos
\ifpdf
  \usepackage[pdftex]{graphicx}
\else
  \usepackage[dvips]{graphicx}
\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Suporte à criação de gráficos proceduralmente na LaTeX:
\usepackage{tikz}
  \usetikzlibrary{arrows,automata,backgrounds,matrix,patterns,positioning,shapes,shadows}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Packages para tabelas
\usepackage{array}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{tabu}
\usepackage{lscape}
\usepackage{colortbl}  
\usepackage{booktabs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Packages ambientes de listas
\usepackage{enumitem}
\usepackage[ampersand]{easylist}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Packages para suporte a ambientes floats, captions, etc.:
\usepackage{float}
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage{caption}
\usepackage{sidecap}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Meus comandos específicos:
% Commando para ``italizar´´ palavras em inglês (e outras línguas!):
\newcommand{\ingles}[1]{\textit{#1}}

% Commando para colocar o espaço correto entre um número e sua unidade:
\newcommand{\unidade}[2]{\ensuremath{#1\,\mathrm{#2}}}
\newcommand{\unidado}[2]{{#1}\,{#2}}

% Produz ordinal masculino ou feminino dependendo do segundo argumento:
\newcommand{\ordinal}[2]{%
#1%
\ifthenelse{\equal{a}{#2}}%
{\textordfeminine}%
{\textordmasculine}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Hifenização específica quando o LaTeX/Babel não conseguirem hifenizar:
\babelhyphenation{Git-Hub}
\usepackage{exsol}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMEÇA O DOCUMENTO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Probability:\\
  The Science of Uncertainty and Data}
\author{MITx 6.431x}
\date{2018/08/28 -- 2018/12/23}
\maketitle
\tableofcontents




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Welcome to 6.431x: Probability --- The Science of Uncertainty and Data!
  (2018/08/28)}
\label{welcome}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Welcome to 6.431x: Unit 0 released}
\label{welcome-course-open}

The course site is now open. We have released Unit 0, which introduces the course
and summarizes the objectives and what you can expect to learn. It also contains
lots of important information that you should read over carefully. We have also
released an Entrance Survey, and will appreciate your help in improving this course.
Unit 1 will be released next Monday.

This is a graduate level version of 6.041x, which has been offered several times,
and we are once more excited to offer this material. We hope that you will find
this course an enriching educational experience, helping you to master the fundamental
concepts and tools of probability theory and its applications.

This is a \emph{challenging} class. It is exactly at the same level, breadth, and
depth as the corresponding residential MIT offering. MIT students typically \emph{spend
  about 12 hours a week} on this subject, and you can expect to need a similar time
commitment, perhaps even a bit more, depending on your background. But even if you
do not have the time to do everything, you may still gain a lot by following just
parts of the course.

We look forward to seeing you in class! And tell your friends about it!

Best wishes,
Prof. John Tsitsiklis, Eren Kizildag (TA), and your course team


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quick info}
\label{welcome-info}

This is a full semester course on the basic tools of probabilistic modeling.
This course covers: 

\begin{itemize}[noitemsep]
\item the general framework of probability models
\item conditional probabilities, independence, and the Bayes rule
\item multiple discrete or continuous random variables
\item expectations, conditional expectations, variance
\item various powerful tools of general applicability, including methods for calculating probabilities and expectations.
\item laws of large numbers
\item the main tools of Bayesian inference methods
\item an introduction to random processes (Poisson processes and Markov chains)
\end{itemize}

The contents of this course are essentially the same as those of the corresponding
MIT class, which has been offered and continuously refined over more than 50 years.
It is a \emph{challenging class}, but will enable you to apply the tools of
probability theory to real-world applications or your research.

If you are new to the course, please review the following features, which are
located in the menu that runs across the top of every page. If this is your
first edX course, you may also consider taking the
\href{https://www.edx.org/course/edx/edx-edxdemo101-edx-demo-1038}{edX Demo course}
first to explore the edX learning experience.

\paragraph{Course Info \& Updates}
You are currently in this tab, where you will find links to useful handouts at the
right side of the page, as well as any updates and announcements regarding the course.

\paragraph{Courseware}
This tab contains the main materials of the course, including lectures, solved problems,
problem sets, and exams. When you return to this site, edX will remember where you
left off, for your convenience.

\paragraph{Discussion}
All of the discussion forums throughout the course can be navigated through the
Discussion tab, but we recommend that you use the in-page discussion boards that are
specially created within each unit to help focus the discussions.

\paragraph{Progress}
This tab allows you to track your progress in the course by providing you with a summary
of your score on each assignment as well as a useful plot of your overall score.

\paragraph{Resources}
In this tab you will find various useful resources, such as a standard normal
table and the textbook excerpts.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Updates}
\label{welcome-updates}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{2018/09/04: Unit 1 Released; Grading Policy; Discussion Forum Guidelines}
\label{welcome-updates-1}

Unit 1 Probabilistic Models and Axioms, has been released.  We will dive into the
heart of the subject right away and learn about the elements of a probabilistic model. 
 
The lecture exercises and the first problem set are due in a week, on Tuesday
Sept 11.  While some parts may appear easy, make sure you understand every single
detail well in order to do well as the course becomes more demanding.  Please also
note that because of the tight schedule, there will be no extensions to any of
the deadlines in this course.
 
After careful considerations, we have also adjusted the grading policy to put more
weight on the timed exams. Furthermore, the grade of one of the eleven problem sets
will also be dropped.  The updated grading policy is posted in Unit 0.  If you have
not already, please make sure you go over Unit 0 carefully, as it contains a lot of
important information. We also appreciate your filling in the Entrance Survey.
 
Finally, we are very excited to see your engagement in the discussion forum already.
Please continue to give your constructive comments and help to each other and to us,
but don’t forget: DO NOT POST OR GIVE AWAY SOLUTIONS on the forum!
 
We are happy to have you with us.

Best wishes,\\
Prof John Tsitsiklis, Eren Kizildag (TA), Karene Chu, and the rest of your course team

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{2018/09/07: Unit 1 due next Tuesday; No solutions on forum please}
\label{welcome-updates-2}

We hope you are enjoying your journey in the world of uncertainty thus far!

The lecture exercises for Unit 1: Probability models and axioms and the associated
Problem Set 1 are due on Tuesday, September 11. We want to emphasize once more that
even though some parts of the problems might appear easy, we strongly recommend you
have a look at the solutions and make sure you fully understood everything, as this
unit serves as the fundamental background for the rest of the course. The solutions
to the problem set will be available after the due date.
 
We are also delighted with the activity on the discussion forum, and we hope you'll
continue the constructive discussion for the rest of the course! Please continue to
give hints and ask questions to lead others towards more understanding, or just help
in any creative way! On the other hand, please be careful not to post or give away
answers. We know this is the beginning of the course and you may not be aware, and
we will try our best to remove answers from posts, but we warn now that if a learner
posts answers repeatedly, we might have to revoke their course access. Please be sure
to review the discussion forum guidelines.
 
Finally, Unit 2: Conditioning and independence and the associated Problem Set 2 will
be released next Monday, September 10; and the due date will be on Tuesday,
September 18.
 
Best wishes,\\
Prof John Tsitsiklis, Eren Kizildag (TA), Dr. Karene Chu, and the rest of your course team 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Unit 0: Overview (2018/08/28)}
\label{ovw0}

Unit 0 is the first unit available in the Courseware. It introduces the course and
summarizes the objectives and what you can expect to learn. It also contains lots
of important information that you should read over carefully.

\paragraph{Course overview; Course introduction, objectives, and study guide}
These sections introduce and overview the course and provide a guide for how to
make the most of the wealth of materials that this course offers.

\paragraph{Syllabus, calendar, and grading policy}
Here you will find an outline of the units of this course, together with release
and due dates. The same information is presented in a calendar format for your
convenience. The grading policy is also explained in detail.

\paragraph{edX tutorial}
This sequence of videos gives a visual tutorial of how to use the basic elements
of the edX platform.

\paragraph{Discussion forum and collaboration guidelines}
This section contains the course's guidelines for collaboration and using the
dicussion forum. Please read them carefully and follow them throughout the course.

\paragraph{Homework mechanics and standard notation}
This section explains how to submit answers to problems and details the standard
notation that should be used throughout the course when entering symbolic responses.
Please read carefully and refer back to these documents when needed.

\paragraph{Textbook information}
This section describes how to access and navigate through the e-reader of excerpts
from the course textbook. There is also information for purchasing a physical copy
of the textbook as well as a link to textbook errata. While this textbook is
recommended, the materials provided by this course are self-contained.

\paragraph{Micromasters, Certificates, and Honor Pledge}
This section provides information on how to earn a verified certificate for this
course, as well as how to obtain the credential for the MITx Micromasters Program
in Statistics and Data Science. You will also be asked to make a pledge to abide
by the EdX Honor Code.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Course overview}
\label{ovw0-co}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Course character and objectives}
\label{ovw0-co-objectives}

Video: \href{https://www.youtube.com/watch?v=OM3iEZGC2-k}{Course character and objectives}
(\href{Unit-0/Overview1\_transcripts.pdf}{transcripts}, \href{Unit-0/Overview1\_slides.pdf}{slides})

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Why study probability?}
\label{ovw0-co-why}

Video: \href{https://www.youtube.com/watch?v=cvVmmboMiyY}{Why study probability?}
(\href{Unit-0/Overview2\_transcripts.pdf}{transcripts}, \href{Unit-0/Overview2\_slides.pdf}{slides})

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Course contents}
\label{ovw0-co-contents}

Video: \href{https://www.youtube.com/watch?v=6EZKb-Lo9IQ}{Course contents}
(\href{Unit-0/Overview3\_transcripts.pdf}{transcripts}, \href{Unit-0/Overview3\_slides.pdf}{slides})


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Course introduction, objectives and study guide}
\label{ovw0-ci}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Introduction}
\label{ovw0-ci-intro}

Welcome to 6.431x, an introduction to probabilistic models, including random
processes and the basic elements of statistical inference.

The world is full of uncertainty: accidents, storms, unruly financial markets,
noisy communications. The world is also full of data. Probabilistic modeling and
the related field of statistical inference are the keys to analyzing data and making
scientifically sound predictions.

The course covers all of the basic probability concepts, including:

\begin{itemize}[noitemsep]
\item multiple discrete or continuous random variables, expectations, and conditional distributions
\item laws of large numbers
\item the main tools of Bayesian inference methods
\item an introduction to random processes (Poisson processes and Markov chains)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Course objectives}
\label{ovw0-ci-obj}

Upon successful completion of this course, you will:

\textbf{At a conceptual level:}

\begin{itemize}[noitemsep]
\item Master the basic concepts associated with \emph{probability models}.
\item Be able to translate models described in words to mathematical ones.
\item Understand the main concepts and assumptions underlying \emph{Bayesian and classical inference}.
\item Obtain some familiarity with the range of \emph{applications of inference methods}.
\end{itemize}

\textbf{At a more technical level:}

\begin{itemize}[noitemsep]
\item Become familiar with basic and common \emph{probability distributions}.
\item Learn how to use \emph{conditioning} to simplify the analysis of complicated models.
\item Have facility manipulating \emph{probability mass functions}, \emph{densities}, and \emph{expectations}.
\item Develop a solid understanding of the concept of \emph{conditional expectation} and its role in inference.
\item Understand the power of \emph{laws of large numbers} and be able to use them when appropriate.
\item Become familiar with the basic inference methodologies (for both \emph{estimation} and \emph{hypothesis testing}) and be able to apply them.
\item Acquire a good understanding of two \emph{basic stochastic processes} (\emph{Bernoulli} and \emph{Poisson}) and their use in modeling.
\item Learn how to formulate simple dynamical models as \emph{Markov chains} and analyze them.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Study guide}
\label{ovw0-ci-sg}

This class provides you with a great wealth of material, perhaps more than you can
fully digest. This “guide" offers some tips about how to use this material.

\paragraph{Start with the overview of a unit, when available.} This will help you
get an overview of what is to happen next. Similarly, at the end of a unit, watch
the unit summary to consolidate your understanding of the “big picture" and of the
relation between different concepts.

\paragraph{Watch the lecture videos.} You may want to download the slides (clean
or annotated) at the beginning of each lecture, especially if you cannot receive
high-quality streaming video. Some of the lecture clips proceed at a moderate speed.
Whenever you feel comfortable, you may want to speed up the video and run it faster, at 1.5x.

\paragraph{Do the exercises!} The exercises that follow most of the lecture clips are
a most critical part of this class. Some of the exercises are simple adaptations of
you may have just heard. Other exercises will require more thought. Do your best to
solve them right after each clip --- do not defer this for later --- so that you can
consolidate your understanding. After your attempt, whether successful or not, do
look at the solutions, which you will be able to see as soon as you submit your own
answers.

\paragraph{Solved problems and additional materials.} In most of the units, we are
providing you with many problems that are solved by members of our staff. We provide
both video clips and written solutions. Depending on your learning style, you may
pick and choose which format to focus on. But in either case, it is important that
you get exposed to a large number of problems.

\paragraph{The textbook.} If you have access to the textbook, you can find more precise
statements of what was discussed in lecture, additional facts, as well as several
examples. While the textbook is recommended, the materials provided by this course
are self-contained. See the “Textbook information" tab in Unit 0 for more details.

\paragraph{Problem sets.} One can really master the subject only by solving problems ---
a large number of them. Some of the problems will be straightforward applications
of what you have learned. A few of them will be more challenging. Do not despair
if you cannot solve a problem – no one is expected to do everything perfectly.
However, once the problem set solutions are released (which will happen on the due
date of the problem set), make sure to go over the solutions to those problems
that you could not solve correctly.

\paragraph{Exams.} The midterm exams are designed so that in an on-campus version,
learners would be given two hours. The final exam is designed so that in an on-campus
version, learners would be given three hours. You should not expect to spend much more
than this amount of time on them. In this respect, those weeks that have exams
(and no problem sets!) will not have higher demands on your time. The level of
difficulty of exam questions will be somewhere between the lecture exercises and
homework problems.

\paragraph{Time management.} The corresponding on-campus class is designed so that
students with appropriate prerequisites spend about 12 hours each week on lectures,
recitations, readings, and homework. You should expect a comparable effort, or more
if you need to catch up on background material. In a typical week, there will be 2
hours of lecture clips, but it might take you 4--5 hours when you add the time spent
on exercises. Plan to spend another 3--4 hours watching solved problems and additional
materials, and on textbook readings. Finally, expect about 4 hours spent on the
weekly problem sets.

\paragraph{Additional practice problems.} For those of you who wish to dive even
deeper into the subject, you can find a good collection of problems at the end of
each chapter of the print edition of the book, whose solutions are available online. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Syllabus, calendar, and grading policy}
\label{ovw0-sy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Syllabus}
\label{ovw0-sy-sy}

\textbf{6.431x Fall 2018 Syllabus}

\begin{itemize}[noitemsep]
\item Unit 0: Overview (released Tue. August 28)
\item Unit 1: Probability models and axioms (released Mon. Sep 3; Sections 1.1--1.2)
  \begin{itemize}[noitemsep]
  \item L1: Probability models and axioms
  \item Problem Set 1 due on Tue Sept 11
  \end{itemize}
\item Unit 2: Conditioning and independence (released Mon. Sept 10; Sections 1.3--1.5)
  \begin{itemize}[noitemsep]
  \item L2: Conditioning and Bayes' rule
  \item L3: Independence
  \item Problem Set 2 due on Tue Sept 18
  \end{itemize}
\item Unit 3: Counting (released Mon. Sept 17; Section 1.6)
  \begin{itemize}[noitemsep]
  \item L4: Counting
  \item Problem Set 3 due on Tue Sept 25
  \end{itemize}
\item Unit 4: Discrete random variables (released Wed. Sept 19; Sections 2.1--2.7)
  \begin{itemize}[noitemsep]
  \item L5: Probability mass functions and expectations
  \item L6: Variance; Conditioning on an event; Multiple r.v.'s
  \item L7: Conditioning on a random variable; Independence of r.v.'s
  \item Problem Set 4 due on Tue Oct 2
  \end{itemize}
\item Exam 1 (Timed) : Covers material from L1 to L7 (released Wed. Oct 3; due on Tue. Oct 9)
\item Unit 5: Continuous random variables (released Mon. Oct 1; Sections 3.1--3.5)
  \begin{itemize}[noitemsep]
  \item L8: Probability density functions
  \item L9: Conditioning on an event; Multiple r.v.'s
  \item L10: Conditioning on a random variable; Independence; Bayes' rule
  \item Problem Set 5 due on Tue. Oct 16
  \end{itemize}
\item Unit 6: Further topics on random variables (released Mon. Oct 15; Sections 4.1--4.3, 4.5)
  \begin{itemize}[noitemsep]
  \item L11: Derived distributions
  \item L12: Sums of r.v.'s; Covariance and correlation
  \item L13: Conditional expectation and variance revisited; Sum of a random number of r.v.'s
  \item Problem Set 6 due on Tue. Oct 23
  \end{itemize}
\item Unit 7: Bayesian inference (released Mon. Oct 22 Sections 3.6, 8.1--8.4)
  \begin{itemize}[noitemsep]
  \item L14: Introduction to Bayesian inference
  \item L15: Linear models with normal noise
  \item L16: Least mean squares (LMS) estimation
  \item L17: Linear least mean squares (LLMS) estimation
  \item Problem Set 7a due on Tue. Oct 30
  \item Problem Set 7b due on Tue. Nov 6
  \end{itemize}
\item Exam 2 (Timed): Covers material from L8 to L17 (released Wed. Nov 1; due on Nov 13)
\item Unit 8: Limit theorems and classical statistics (released Mon. Nov 5; Sections 5.1--5.4, pp. 466--475)
  \begin{itemize}[noitemsep]
  \item L18: Inequalities, convergence, and the Weak Law of Large Numbers
  \item L19: The Central Limit Theorem (CLT)
  \item L20: An introduction to classical statistics
  \item Problem Set 8 due on Tue. Nov 27
  \end{itemize}
\item Unit 9: Bernoulli and Poisson processes (released Tue. Nov 14; Sections 6.1-6-2)
  \begin{itemize}[noitemsep]
  \item L21: The Bernoulli process
  \item L22: The Poisson process
  \item L23: More on the Poisson process
  \item Problem Set 9 due on Tue. Dec 4
  \end{itemize}
\item Unit 10: Markov chains (released Tue. Nov 26; Sections 7.1--7.4)
  \begin{itemize}[noitemsep]
    \item L24: Finite-state Markov chains
    \item L25: Steady-state behavior of Markov chains
    \item L26: Absorption probabilities and expected time to absorption
    \item Problem Set 10 due on Tue. Dec 11
  \end{itemize}
\item Final Exam (Timed) (released Wed. Dec 12; due on Sun. Dec 23)
\end{itemize}

\textbf{Note: Problem set and exam due dates are at the end of the specified date,
  at 23:59 UTC.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Calendar}
\label{ovw0-sy-calendar}

\begin{figure}[H]
  \begin{center}
    %\caption{}
    \label{fig:calendar}
    \fbox{\includegraphics[scale=0.8]{imagens/calendar.png}}
  \end{center}
\end{figure}

Notes:

\begin{itemize}[noitemsep]
\item The due dates for the weekly problem sets and the exams are fixed and cannot
  be changed or modified for any individuals. Please plan accordingly.
\item Problem set and exam due dates are at the end of the specified date, at 23:59 UTC.
\item The calendar above shows only Tuesdays, Wednesdays, and Thursdays, since these
  are the only days of the week when materials will be released or due, except
  the final exam.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Gradingp policy}
\label{ovw0-sy-grades}

\paragraph{Grading policy} Your overall score in this class will be a weighted
average of your scores for the different components, with the following weights:

\begin{itemize}[noitemsep]
\item 20\% for the lecture exercises (divided equally among the 26 lectures)
\item 20\% for the problem sets (divided equally among 11 problem sets)
\item 18\% for the first midterm exam (timed)
\item 18\% for the second midterm exam (timed)
\item 24\% for the final exam (timed)
\end{itemize}

To earn a verified certificate for this course, you will need to obtain an
\emph{overall score} of 60\% or more of the maximum possible overall score.

Note that not every problem set or set of lecture exercises will have the same
number of raw points. For example, Problem Set 1 may have 30 points and Problem
Set 2 may have 35 points. However, each one receives the same weight for the
purposes of calculating your overall score.

As an illustrative example, if you receive 20 points out of 30 on Problem Set 1,
this will contribute $\frac{20}{30} \times \frac{20\%}{11} = 1.21\%$ to your overall score. Similarly, if
you receive 30 points out of 35 on Problem Set 2, this will contribute
$\frac{30}{35} \times \frac{20\%}{11} = 1.56\%$ to your overall score.

Under the “Progress" tab at the top, you can see your score broken down for each
assignment, as well as a summary plot.

\paragraph{Timed Exams} The 2 midterm exams and one final exam are \emph{timed exams}.
This means that each exam is available for approximately a week, but once you open
the exam, there is a limited amount of time (48 hours), counting from when you start,
within which you must complete the exam. Please plan in advance for the exams. If you
do not complete the whole exam during the allowed time, you will miss the points
associated with the questions that have not been answered. The exams are designed
to assess your knowledge. There are no extensions granted to these deadlines.
You can find the exam dates on the calendar on the previous page. Note that the
timed exams cannot be completed using the edX mobile app.

\paragraph{MITx Committment to Accessibility} If you have a disability-related
request regarding accessing an MITx course, including exams, please contact the course
team as early in the course as possible (at least 2 weeks in advance of exams opening)
to allow us time to respond in advance of course deadlines. Requests are reviewed
via an interactive process to meet accessibility requirements for learners with
disabilities and uphold the academic integrity for MITx.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{edX Tutorial}
\label{ovw0-edx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Basics}
\label{ovw0-edx-basics}

Video: \href{https://www.youtube.com/watch?v=OEoXaMPEzfM}{edX Basics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Courseware navigation}
\label{ovw0-edx-navigation}

Video: \href{https://www.youtube.com/watch?v=7m8pab1MfYY}{Courseware navigation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Top-level navigation}
\label{ovw0-edx-top-nav}

Video: \href{https://www.youtube.com/watch?v=pflzzK47wEo}{Top-level navigation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Discussion forums}
\label{ovw0-edx-forum}

Video: \href{https://www.youtube.com/watch?v=Q-rY8DIwYgg}{Discussion forums}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Summary}
\label{ovw0-edx-}

Video: \href{https://www.youtube.com/watch?v=RQdyRhHDlRo}{Summary}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion forum and collaboration guidelines}
\label{ovw0-forum}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Discussion forum guidelines}
\label{ovw0-forum-guide}

\paragraph{Discussion forum overview} The course provides an online discussion
forum for you to communicate with the course
team and other learners. You may access the forum through the “Discussion" tab at the
top of the page, as well as through many embedded discussions within each unit.
We recommend using the embedded discussions within each unit to discuss topics related
to a specific unit's materials, whether it's lectures, solved problems, or problem
set problems. Please see the guidelines below for more information on how to use
these embedded discussions.

For other more general discussions, you may use the “Discussion" tab at the top of
the page. When creating a new post, \emph{please choose one of the following categories that
best describes your post}:

\begin{itemize}[noitemsep]
\item \emph{Introductions}: Introduce yourself to your fellow learners and find
  out more about them!
\item \emph{Micromasters}: Ask questions related to the
  \href{https://www.edx.org/micromasters/mitx-statistics-and-data-science}{MITx
    Micromaster Program in Statistics and Data Science} and meet other Micromasters
  fellows!
\item \emph{Course Feedback}: Let the course team know how you are finding the course,
  what you think works well, and what you would like to see improved.
\item \emph{Technical Problems}: Let the course team know about any technical issues
  you are dealing with (e.g., playing videos, entering answers, etc).
\item \emph{General}: Other general discussions.
\end{itemize}

\paragraph{Discussion forum guidelines} The discussion forum is the main way for you
to communicate with the course team and other learners. We hope it contributes to a
sense of community and serves as a useful resource for your learning. Here are some
guidelines to help you successfully navigate and interact on the forum:

\begin{itemize}[noitemsep]
\item \emph{Use discussion while working through the material}. Beginning with Unit 1,
  each lecture will contain an embedded discussion located at the bottom of the lecture
  overview clip, which is the first or second clip of that lecture sequence. You should
  discuss anything related to that lecture's video clips or exercises there. Click
  “Show Discussion" to see all discussions associated with the lecture, and click
  “Add a Post" to post a new topic. In addition, every solved problem and problem set
  problem will have its own embedded discussion located at the bottom of their respective
  pages. As with the lecture discussions, click “Show Discussion" and “Add a Post" to
  see and create discussion topics related to that specific problem. We recommend that
  you use these in-page discussion boards to help focus discussions on specific topics.
\item \emph{Use informative topic titles and tags}. To make it easier to identify relevant
  discussion topics, please use informative titles and tags when creating a new discussion
  topic. We suggest using titles or tags that are as informative as possible, e.g.,
  “lecture X, exercise Y on topic W, clarify part Z"
\item \emph{Be very specific}. Provide as much information as possible about what you need
  help for: Which part of what problem or video? Why do you not understand the question?
  Do you need help understanding a particular concept? What have you tried doing so far?
  Use a descriptive title to your post. This will attract the attention of other learners
  having the same issue.
\item \emph{Observe the honor code}. We encourage collaboration and help, but please do
  not ask for nor post problem solutions.
\item \emph{Upvote good posts}. This applies to questions and answers. Click on the green
  plus button so that good posts can be found more easily.
\item \emph{Search before asking}. The forum can become hard to use if there are too many
  threads, and good discussions happen when people participate in the same thread. Before
  asking a question, use the search feature by clicking on the magnifying glass on the
  left-hand side.
\item \emph{Write clearly}. We know that English is a second language for many of you but
  correct grammar will help others to respond. Avoid ALL CAPS, abbrv of wrds (abbreviating
  words), and excessive punctuation!!!!
\end{itemize}

\paragraph{Please Introduce Yourself!} Let's get started by introducing yourselves on the
discussion forum. A lot of the learning in this class will happen in your interactions with
each other. Click on the post titled “Introduce yourself!" below, and respond to it by
telling everyone your name, where you are from, why you are taking this course, and whatever
else you would like to share! Your post will be indexed in the “Introductions"
category in the forum.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Collaboration guidelines}
\label{ovw0-collab-guide}

We encourage you to interact with your fellow learners and engage in active discussion
about the course. Please use the guidelines below for acceptable collaboration.
The staff will be proactive in removing posts and replies in the discussion forum
that have stepped over the line.

\begin{itemize}[noitemsep]
\item Given a problem, it is ok to discuss the general approach to solving the problem.
\item You can work jointly to come up with the general steps for the solution.
\item It is ok to get a hint, or several hints for that matter, if you get stuck while
  solving a problem.
\item You should work out the details of the solution yourself.
\item It is not ok to take someone else's solution and simply copy the answers
  from their solution into your checkboxes.
\item It is not ok to take someone else's formula and plug in your own numbers
  to get the final answer.
\item It is not ok to post answers to homework and lab problems before the
  submission deadline.
\item It is not ok to look at a full step-by-step solution to a problem before
  the submission deadline.
\item It is ok to have someone show you a few steps of a solution where you have
  been stuck for a while, provided of course, you have attempted to solve it
  yourself without success.
\item After you have collaborated with others in generating a correct solution,
  a good test to see if you were engaged in acceptable collaboration is to make
  sure that you are able to do the problem on your own. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Homework mechanics and standard notation}
\label{ovw0-hw}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Checking and submitting an answer}
\label{ovw0-hw-checking}

\paragraph{Checking and submitting an answer} For each problem, you will have between
2 to 5 attempts to submit an answer, with the exception of problems where an attempt
essentially reveals the answer (e.g., True/False questions), for which you will be
limited to a single attempt.

To submit your answer, click the “Submit " button. This will automatically submit
the problem for grading purposes, and the edX platform is able to verify your answer
and give you immediate feedback as to whether or not your answer is correct. To save
your answer without submitting it for grading purposes, click the “Save" button.
Your answer will be restored when you return to the problem.

The number of attempts allowed as well as the number of attempts you've already made
will always be visible on a problem's page at the bottom, next to the “Check" button.
Please note that for problems consisting of multiple parts, hitting the button will count
as an attempt for all parts of the problem. Unfortunately, it is not possible to submit
answers for one part at a time.

For lecture exercises, a “Show Answer(s)" button will appear immediately after you
submit the correct answer or use all of your attempts. Clicking this button will
reveal the correct answers and solutions.

For homework problems, the “Show Answer(s)" button will appear after the due date
of the homework.

You are strongly encouraged to look at the solutions even if your answer is correct.

\paragraph{Answer formats} This course will use several answer formats:

\begin{itemize}[noitemsep]
\item Multiple choice: Select the correct option from the dropdown menu or radio buttons.
\item Numerical answers: Enter a number, either in decimal (e.g., '3.14159') or fractional
  form (e.g., '22/7'). Do not enter any non-numerical letters or symbols. To account for
  rounding, the system will accept a range of answers as correct. Unless otherwise specified
  in the problem, the default tolerance range will be +/-3\% of the correct answer.
\item Symbolic answers: Some problems will ask for a symbolic answer (e.g., 'n*(n+1)/2').
  See the next section on “Standard notation" for details on how to submit such answers. 
\end{itemize}

Below are some example problems for you to familiarize yourselves with how these problem
types work with different number of attempts. These problems are not graded and have no
impact on your grade.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Standard notation}
\label{ovw0-hw-standard}

Many exercises and problems throughout the course will ask you to provide an
algebraic answer in terms of symbols. Please follow the guidelines below when
entering your responses. Below your answer textbox, the system will also display,
in a "pretty" format, what it has interpreted your input to be. However, this display
is not perfect (for example, it does not catch all cases of missing close parentheses)
so please also check your text input carefully.

\begin{itemize}[noitemsep]
\item Symbols are case-sensitive: $a$ and $A$ are different --- make sure to use
  the correct case as specified in the problem
\item Parentheses: make sure that your parentheses are properly balanced --- each
  open parenthesis should have a matching close parenthesis!
\item Elementary arithmetic operations: use the symbols + , - , * , / for addition,
  subtraction, multiplication, and division, respectively
\item $1+bc-d/e$ should be entered as 1+b*c-d/e
\item For multiplication, use * explicitly:
  \begin{itemize}[noitemsep]
  \item in the example above, enter b*c; do NOT enter bc
  \item for $2n(n+1)$, enter 2*n*(n+1); do NOT enter 2n(n+1)
  \item although the "pretty" display underneath your answer looks correct if
    you do not include * s, your answer will be marked incorrect!
  \end{itemize}
\item Exponents: use the symbol \^\ to denote exponentiation
  \begin{itemize}[noitemsep]
  \item $2^n$ should be entered as 2\^\,n
  \item $x^{n+1}$ should be entered as x\^\,(n+1)
  \end{itemize}
\item Square root: use the string of letters sqrt , followed by enclosing what
  is under the square root in parentheses
  \begin{itemize}[noitemsep]
  \item $\sqrt{-1}$ should be entered as sqrt(-1)
  \end{itemize}
\item Mathematical constants: use the symbol e for the base of the natural logarithm,
  $e$; use the string of letters pi for $\pi$
  \begin{itemize}[noitemsep]
  \item $e^{i\pi}+1$ should be entered as e\^\,(i*(pi))+1
  \end{itemize}
\item Order of operations: 1) parentheses, 2) exponents and roots,
  3) multiplication and division, 4) addition and subtraction
  \begin{itemize}[noitemsep]
  \item $\displaystyle \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ should be entered
    as (1/sqrt(2*(pi)))*e\^\,(-(x\^\,2)/2)
  \item a/b*c is interpreted as $\frac{a}{b}c$
  \item enter a/(b*c) for $\frac{a}{bc}$
  \item When in doubt, use additional parentheses to remove
    possible ambiguitites
  \end{itemize}
\item Natural logarithm: although in lectures and solved problems we will
  sometimes use the notation "log" (instead of "ln"), you should use the
  string of letters ln , followed by the argument enclosed in parentheses
  \begin{itemize}[noitemsep]
  \item $\ln{(2x)}$ should be entered as ln(2*x)
  \end{itemize}
\item Trigonometric functions: use the usual 3-letter symbols to denote
  the standard trigonometric functions
  \begin{itemize}[noitemsep]
  \item $\sin{(x)}$ should be entered as sin(x)
  \end{itemize}
\item Greek letters: use the Latin-character name to denote each Greek letter
  \begin{itemize}[noitemsep]
  \item $\lambda e^{-\lambda t}$ should be entered as lambda*e\^\,(-lambda*t)
  \item $\mu \alpha \theta$ should be entered as mu*alpha*theta
  \end{itemize}
\item Factorials, permutations, combinations: you will not need enter these
  for any symbolic answers; do NOT use ! in your answers as it will not be evaluated correctly!
\end{itemize}

\begin{figure}[H]
  \begin{center}
    \caption{Standard Notation Summary: 1}
    \label{fig:std-ntt-1}
    \fbox{\includegraphics[scale=0.5,angle=90,origin=c]{imagens/standard-notation-1.png}}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \caption{Standard Notation Summary: 2}
    \label{fig:std-ntt-2}
    \fbox{\includegraphics[scale=0.6,angle=90,origin=c]{imagens/standard-notation-2.png}}
  \end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Textbook information}
\label{ovw0-book}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Textbook}
\label{ovw0-book-textbook}

The class follows closely the text Introduction to Probability, 2nd edition, by
Bertsekas and Tsitsiklis, Athena Scientific, 2008; see the
publisher's website or
Amazon.com for more information.

While this textbook is recommended, the materials provided by this course are
self-contained. Furthermore, the publisher has made available, for the purposes
of this class, the summary tables that are included in the text. These can be found
under the “Resources" tab, or directly by following
\href{https://courses.edx.org/courses/course-v1:MITx+6.431x+3T2018/pdfbook/0/chapter/1/1}{this link}.
In various places
within the courseware, there will also be links to specific sections and pages to
the excerpts from the textbook relevant to the material at hand. These links will
also take you to the e-reader, jumping directly to the specific sections and pages.

To adjust the zoom level in the e-reader, click the '+' and '-' buttons at the
top-right to zoom in and out, respectively. Or, choose a specific zoom level using
the drop-down menu. Depending on your operating system and web browser, you may
encounter occasional artifacts or imperfect rendering of some formulas. Please try
adjusting the zoom level to find the one that gives the best readability. We recommend
using Firefox as it renders the text most accurately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ordering and other information}
\label{ovw0-book-ordering}

The class follows closely the text Introduction to Probability, 2nd Ed., by
Bertsekas and Tsitsiklis. If interested in purchasing a copy of the
\href{http://athenasc.com/probbook.html}{textbook},
it is available through
\href{http://www.amazon.com/Introduction-Probability-Edition-Dimitri-Bertsekas/dp/188652923X/}{Amazon.}

Textbook errata can be found \href{http://athenasc.com/prob-errata\_2ndedition.pdf}{here}.
Ignore “Corrections to the 1st and 2nd printing." These do not apply to the currently
available printed version.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Micromasters, Certification, and Honor Pledge}
\label{ovw0-mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Micromasters}
\label{ovw0-mm-mm}

Video: \href{https://www.youtube.com/watch?v=Xe97K6cLQZ8}{MITs Micromasters in Statistics and Data Science}

This course is part of the
\href{https://www.edx.org/micromasters/mitx-statistics-and-data-science}{MITx
  Micromaster Program in Statistics and Data Science}. Welcome to the program!

\paragraph{About the Program} The MITx Micromasters program in Statistics and Data
Science is comprised of four EdX courses and a virtually proctored exam that will
provide you with the foundational knowledge essential to understanding the methods
and tools used in data science, and hands-on training in data analysis and machine
learning. You will dive into the fundamentals of probability and statistics, as well
as learn, implement, and experiment with data analysis techniques and machine learning
algorithms. This program will prepare you to become an informed and effective practitioner
of data science who adds value to an organization and will also accelerate your path
towards an MIT PhD or a Master's at other universities.

Anyone can enroll in the Micromasters program just as in any EdX courses. It is designed
for learners who want to acquire sophisticated and rigorous training in data science
without leaving their day job but also without compromising quality. There is no
application process. To excel in the entire program, make sure you learn the
foundational material covered in this course. You will also need some knowledge of
matrices and proficiency in Python programming.

\paragraph{What You'll Learn} You'll lean about:

\begin{itemize}[noitemsep]
\item Master the foundations of data science, statistics, and machine learning
\item Analyze big data and make data-driven predictions through probabilistic
  modeling and statistical inference; identify and deploy appropriate modeling
  and methodologies in order to extract meaningful information for decision making
\item Develop and build machine learning algorithms to extract meaningful information
  from seemingly unstructured data; learn popular unsupervised learning methods,
  including clustering methodologies and supervised methods such as deep neural networks
\item Master techniques in modern data analysis to leverage big datasets; use python
  and R skillfully to analyze data
\end{itemize}

\paragraph{How to earn the Micromasters credential} To earn the MITx Micromasters
credential in statistics and data science, you must successfully pass and receive
a Verified Certificate in each of the 4 courses listed below and pass the final
Capstone Exam:

\begin{itemize}[noitemsep]
\item 6.431x Probability–the Science of Uncertainty and Data
\item 14.310Fx Data Analysis in Social Sciences
\item 18.6501x Fundamentals of Statistics
\item 6.86x Machine Learning with Python–From Linear Models to Deep Learning
\item DS-CFx Captone Exam in Statistics and Data Science 
\end{itemize}

All the courses are taught by MIT faculty at a similar pace and level of
rigor as an on-campus course at MIT.

\paragraph{More information} If you are interested in the Micromasters program,
visit \href{https://www.edx.org/micromasters/mitx-statistics-and-data-science}{https://www.edx.org/micromasters/mitx-statistics-and-data-science}.
For more detail on this program and credit pathways, please visit the \href{https://micromasters.mit.edu/ds}{MITx
  Micromasters Portal}, which includes a “Contact us" link at the very bottom left.
You may also find the \href{https://micromasters.mit.edu/ds/faq/}{FAQ} helpful.
Finally, you can start connecting with fellow
Micromasters learners on the discussion forum! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Certification}
\label{ovw0-mm-certification}

To earn a Verified Certificate in this course, you need to:

\begin{itemize}[noitemsep]
\item \href{https://courses.edx.org/verify_student/upgrade/course-v1:MITx+6.431x+3T2018/}{Upgrade your status}
  to be a verified learner - The fee is \$300.
\item \href{https://support.edx.org/hc/en-us/articles/206503858-Best-Practices-for-edX-Photo-Verification}{Verify your identity} - ID Verification.
\item Pass the course - at least 60\% on your final grade.
\end{itemize}

You have limited time to switch to a Verified Certificate learner – you should
get ID Verified as soon as you register as a Verified learner. See the EdX FAQ
for more details on certificates.

Note: It is your responsibility to make sure that your ID verification is valid
during the whole course.

A verified certificate indicates that you have successfully completed the course,
but will not include a specific grade. Certificates are issued by edX under the
name of MITx and are delivered online through your dashboard on edx.org.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EdX Honor Code Pledge}
\label{ovw0-mm-honor}

By enrolling in an EdX course, you have already agreed with the EdX Honor Code,
which means that you will do the following:

\begin{itemize}[noitemsep]
\item Complete all graded material (graded assignments and exams) with your
  own work and only your own work. You will not submit the work of any other
  person or have anyone else submit work under your name.
\item Maintain only one user account and not let anyone else use your username
  and/or password. Having two user accounts registered in this course will
  constitute cheating. Not engage in any activity that would dishonestly improve
  your results, or improve or hurt the results of others."."Not collaborate with
  anyone other than staff on the exam questions. This means comparing answers,
  working as teams, or sharing answers in any way.
\item Not post answers to any problems that are used to assess learner performance.
\item Always be polite and respectful when communicating across the platform
  (with other learners and the staff).
\end{itemize}

We will strictly enforce this honor code pledge. Learners found violating this
pledge will be dealt with directly. If we become aware of any suspicious activity
we reserve the right to remove credit, not award a certificate, revoke a certificate,
ban from this and other courses in the MITs Micromasters Program in Statistics and
Data Science as well as notify edX for other actions. We take academic honesty very,
very seriously at MIT. With the introduction of the Micromasters Credential, the
importance of honesty in work has been elevated to a much higher level than before.
We will diligently monitor this and be very proactive. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Entrance survey}
\label{ovw0-survey}

For us to offer the best course experience possible, we'd like to ask you
to answer a few questions about yourself: \href{survey.pdf}{Entrance Survey}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Unit 1: Probability models and axioms (2018/09/03)}
\label{un1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lecture 1: probability models and axioms}
\label{un1-lec1}

Atention: Exercises due Sep 11, 2018 20:59:59 -03.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Motivation}
\label{un1-lec1-motiv}

Video: \href{https://d2f1egay8yehza.cloudfront.net/MIT6041XT114-V036600\_DTH.mp4}{Motivation}

Let's face it.
Life is uncertain.
But one thing is certain.
We need a way to make predictions and make decisions
under uncertainty.
Probabilistic models can help you answer questions, such as:

\begin{itemize}[noitemsep]
\item What are the odds that there will
be a long line at the supermarket checkout counter?
\item How likely is it that my GPS device is off
by more than 10 meters?
\item What are the odds that I will have a car accident next year?
\item How likely is it that the air traffic control radar will
miss the approaching plane?
\item Should I invest in the stock market now or wait?
\item Can I use a probabilistic model of social networking data
to create a marketing campaign?
\item How do we use a statistical model
to decide if a medical treatment is effective?
\item How do I model the huge amounts of data that are now
becoming available in so many different fields, big data,
as they call it, and extract useful information?
\end{itemize}

I am \emph{John Tsitsiklis}.
And I'm \emph{Patrick Jaillet}.
Our mission in this class is to give you the tools
to model and analyze uncertain situations no matter what
your discipline.

To do that, we will use the language and precision
of mathematics, but we will also build your intuition.
This is an ambitious class.
The online version is at \emph{the same level as the one offered
  to MIT students}.

It covers a lot of material.
Beyond the basics, you will learn about random processes
and about extracting information from data.
In the end, you will be able to make much better sense
of the uncertainty around you.
The rewards are certain to come.

Let's face it.
Life is uncertain.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Overview and slides}
\label{un1-lec1-overview}

This lecture sequence introduces the basic structure of probability models,
including the sample space and the axioms that any probabilistic model should obey,
together with some consequences of the axioms and some simple examples.

Video: \href{https://www.youtube.com/watch?v=qgwfz1FCPCc}{Lecture 01: Probability
  Models and Acioms} (%
\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_overview\_transcripts.pdf}{transcripts},
\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_overview\_slides.pdf}{slides},
\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_overview\_annotated\_slides.pdf}{annotated slides}%
)\footnote{The same material, in live lecture hall format, can be found \href{http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/video-lectures/lecture-1-probability-models-and-axioms/}{here} and \href{http://www.youtube.com/watch?v=j9WZyLZCBzs}{here}.} \footnote{You can also take this occasion
  to review some concepts related to sets (especially De Morgan's laws), sequences,
  and infinite series, by watching the “Mathematical background" sequence of clips.} \footnote{More
  information is given in the text:
  \begin{itemize}[noitemsep]
  \item Sets: Section 1.1
  \item Probabilistic models: Section 1.2
  \end{itemize}
  }

Welcome to the first lecture of this class.
You may be used to having a first lecture devoted to
general comments and motivating examples.
This one will be different.

We will dive into the heart of the subject right away.
In fact, today we will accomplish a lot.
By the end of this lecture, you will \emph{know about all of the
elements of a probabilistic model}.

\textbf{A probabilistic model is a quantitative description of a
situation, a phenomenon, or an experiment whose outcome is uncertain}.
Putting together such a model involves two key steps:

\begin{itemize}
\item First, we need to describe the possible outcomes of the
  experiment. This is done by specifying a so-called \emph{sample space}.
\item Second, we specify a \emph{probability law}, which assigns
  probabilities to outcomes or to collections of outcomes.
\end{itemize}

The probability law tells us, for example, whether one
outcome is much more likely than some other outcome.

Probabilities have to satisfy certain basic properties in
order to be meaningful.
These are the \emph{axioms of probability theory}.
For example probabilities cannot be negative.
Interestingly, there will be very few axioms, but they are
powerful, and we will see that they have lots of
consequences.
We will see that they imply many other properties that
were not part of the axioms.

We will then go through a couple of very simple examples
involving models with either \emph{discrete}
or \emph{continuous} outcomes.
As you will be seeing many times in this class, discrete
models are conceptually much easier.
Continuous models involve some more sophisticated concepts,
and we will point out some of the subtle issues that arise.
And finally, we will talk a little bit about the big
picture, about the role of probability theory, and its
relation with the real world.

\begin{figure}[H]
  \begin{center}
    \caption{Objectives of Lecture 1}
    \label{fig:obj-lec-1}
    \fbox{\includegraphics[scale=0.5]{imagens/unit-1/l01-001.png}}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sample space}
\label{un1-lec1-sample-space}

Video: \href{https://www.youtube.com/watch?v=qFBSUg59bWg}{Sample space}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_1\_transcripts.pdf}{transcript})

Putting together a probabilistic model ---
that is, a model of a random phenomenon or a random
experiment --- involves two steps.

\begin{itemize}
\item First step, we describe the \emph{possible outcomes} of the
  phenomenon or experiment of interest.
\item Second step, we describe our beliefs about the \emph{likelihood
  of the different possible outcomes} by specifying a \emph{probability law}.
\end{itemize}
Here, we start by just talking about the first step, namely,
the description of the possible outcomes of the
experiment.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-002.png}}
  \end{center}
\end{figure}

So we carry out an experiment.
For example, we flip a coin.
Or maybe we flip five coins simultaneously.
Or maybe we roll a die.

Whatever that experiment is, it has a number of possible
outcomes, and we start by \emph{making a list of
the possible outcomes} ---
or, a better word, instead of the word "list", is to use the
word "set", which has a more formal mathematical meaning.

So we create a set that we usually
denote by capital omega, $\Omega$.
That set is called the \emph{sample space} and is the set of \textbf{all
possible outcomes of our experiment}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-003.png}}
  \end{center}
\end{figure}

The elements of that set should have certain
properties. Namely, the elements should be \textbf{mutually exclusive} and
\textbf{collectively exhaustive}.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-004.png}}
  \end{center}
\end{figure}

What does that mean?
Mutually exclusive means that, if at the end of the
experiment, I tell you that this outcome happened, then it
should not be possible that this outcome also happened.
At the end of the experiment, there can \emph{only be one of the
outcomes that has happened}.

Being collectively exhaustive means something else --- that,
together, all of these \emph{elements of the set exhaust
all the possibilities}.
So no matter what, at the end, you will be able to point to
one of the outcomes and say, that's the one that occurred.

To summarize:
this set should be such that, at the end of the experiment,
you should be always able to \emph{point to one, and exactly one,
of the possible outcomes} and say that this is the outcome
that occurred.

Physically \emph{different outcomes should be distinguished in the
sample space} and correspond to distinct points.
But when we say physically different
outcomes, what do we mean?
We really mean \emph{different in all relevant aspects} but
perhaps not different in irrelevant aspects.
Let's make more precise what I mean by that by looking at a
very simple, and maybe silly, example,
which is the following.

Suppose that you flip a coin and you see whether it
resulted in heads or tails.
So you have a perfectly legitimate sample space for
this experiment which consists of just two points ---
heads and tails:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-005.png}}
  \end{center}
\end{figure}

Together these two outcomes \emph{exhaust all possibilities}.
And the two outcomes are \emph{mutually exclusive}.
So this is a very legitimate sample space for this
experiment.

Now suppose that while you were flipping the coin, you
also looked outside the window to check the weather.
And then you could say that my sample space is really, heads,
and it's raining.
Another possible outcome is heads and no rain.
Another possible outcome is tails, and it's raining, and,
finally, another possible outcome is tails and no rain.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-006.png}}
  \end{center}
\end{figure}

This set, consisting of four elements, is also a perfectly
legitimate sample space for the experiment
of flipping a coin.
The elements of this sample space are mutually exclusive
and collectively exhaustive.
Exactly one of these outcomes is going to be true, or will
have materialized, at the end of the experiment.

So which sample space is the correct one?
This sample space, the second one, involves
some \emph{irrelevant details}.
So the preferred sample space for describing the flipping of
a coin, the preferred sample space is the simpler one, the
first one, which is sort of at the \emph{right granularity}, \textbf{given
  what we're interested in}.

But ultimately, the question of which one is \emph{the right
sample space depends on what kind of
questions you want to answer}.
For example, if you have a theory that the weather
affects the behavior of coins, then, in order to play with
that theory, or maybe check it out, and so on, then, in such
a case, you might want to work with the second sample space.

This is a common feature in all of science.
Whenever you put together a model, you need to decide how
detailed you want your model to be.
And \emph{the right level of detail is the one that captures those
  aspects that are relevant and of interest to you}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Sample Space}
\label{un1-lec1-exec-sample-space}

\begin{exercise}
  \textbf{Sample space}\\
  For the experiment of flipping a coin, and for each one of the following choices,
  determine whether we have a legitimate sample space:

  $\Omega = \{\text{Heads and it is raining, Heads and it is not raining, Tails}\}$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] Yes
  \item[$\bigcirc$] No
  \end{itemize}

  $\Omega = \{\text{Heads and it is raining, Tails and it is not raining, Tails}\}$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] Yes
  \item[$\bigcirc$] No
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sample space examples}
\label{un1-lec1-sample-space-exs}

Video: \href{https://www.youtube.com/watch?v=iUF5n3Ou7Sc}{Sample space examples}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_2\_transcripts.pdf}{transcripts})

Let us now look at some examples of sample spaces.
\emph{Sample spaces are sets}. And a set can be:

\begin{itemize}
\item Discrete or continuous
\item Finite or infinite
\end{itemize}

Let us start with a simpler case in which we have a sample
space that is discrete and finite.
The particular experiment we will be
looking at is the following.
We take a very special die, a tetrahedral die.
So it's a die that has four faces numbered from 1 up 4.
We roll it once.
And then we roll it twice [again].

Were not dealing here with two probabilistic experiments.
We're dealing with a single probabilistic experiment that
involves two rolls of the die within that experiment.
What is the sample space of that experiment?

Well, one possible
representation is the following:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-007.png}}
  \end{center}
\end{figure}

We take note of the result of the first roll.
And then we take note of the result of the second roll.
And this gives us a pair of numbers.
Each one of the possible pairs of numbers corresponds to one
of the little squares in this diagram.

For example, if the first roll is 1 and the second is also 1,
then this particular outcome has occurred:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-008.png}}
  \end{center}
\end{figure}

If the first roll is it 2 and the second is a 3, then this
particular outcome occurs:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-009.png}}
  \end{center}
\end{figure}

If the first roll is a 3 and then the next one is a 2, then
this particular outcome occurs:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-010.png}}
  \end{center}
\end{figure}

Notice that these two outcomes are pretty closely related.
In both cases, we observe a 2 and we observe a 3.
But we distinguish those two outcomes because in those two
outcomes, the 2 and the 3 happen in different order.
And the order in which they appear may be a detail which
is of interest to us.
And so we make this distinction
in the sample space.
So we keep the (3, 2) and the (2, 3) as separate outcomes.

Now this is a case of a model in which \emph{the probabilistic
experiment can be described in phases or stages}.
We could think about rolling the die once and then going
ahead with the second roll.
So we have two stages.

A very useful way of describing the sample space of
experiments ---
whenever we have an experiment with several stages, either
real stages or imagined stages --- it is by providing
a \emph{sequential description in terms of a tree}.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-011.png}}
  \end{center}
\end{figure}

So a diagram of this kind, we call it a tree.
You can think of this as the \emph{root} of the tree
from which you start.
And the endpoints of the tree, we usually
call them the \emph{leaves}.

So the experiment starts.
We carry out the first phase, which in this case is the
first roll.
And we see what happens.
So maybe we get a 2 in the first roll.
And then we take note of what happened in the second roll.
And maybe the result was a 3.
So we follow this branch here.
And we end up at this particular leaf, which is the
leaf associated with the outcome 2, 3:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-012.png}}
  \end{center}
\end{figure}

Notice that in this tree we once more have a distinction.
The outcome 2 followed by a 3 is different from the outcome
3 followed by a 2, which would correspond to this particular
place in the diagram:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-013.png}}
  \end{center}
\end{figure}

In both cases, we have 16 possible outcomes.
4 times 4 makes 16.
And similarly, if you count here, the number of leaves is
equal to 16.

The previous example involves a sample space that was
discrete and finite.
There were only 16 possible outcomes.
But sample spaces can also be infinite.
And they could also be continuous sets.
Here's an example of an experiment that involves a
continuous sample space.

So we have a rectangular target
which is the unit square:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-014.png}}
  \end{center}
\end{figure}

And you throw a dart on that target.
And suppose that you are so skilled that no matter what,
when you throw the dart, it always
falls inside the target:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-015.png}}
  \end{center}
\end{figure}

Once the dart hits the target, you record the coordinates x
and y of the particular point that resulted
from your dart throw.
And we record x and y with \emph{infinite precision}.
So x and y are real numbers.
So in this experiment, the sample space is just the set
of x, y pairs that lie between 0 and 1 (inclusive):
$\Omega = \left\{(x, y)\quad |\quad 0 \le x, y \le 1\right\}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Tree representations}
\label{un1-lec1-exec-tree-rep}

\begin{exercise}
  \textbf{Tree representations}\\
  Paul checks the weather forecast. If the forecast is good, Paul will go out for
  a walk. If the forecast is bad, then Paul will either stay home or go out. If
  he goes out, he might either remember or forget his umbrella. In the tree diagram
  below, identify the leaf that corresponds to the event that the forecast is bad
  and Paul stays home.

  \begin{figure}[H]
    \begin{center}
      %\caption{}
      %\label{fig:}
      \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-016.png}}
    \end{center}
  \end{figure}

  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] 1
  \item[$\bigcirc$] 2
  \item[$\bigcirc$] 3
  \item[$\bigcirc$] 4
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Probability axioms}
\label{un1-lec1-prob-axioms}

Video: \href{https://www.youtube.com/watch?v=2J5Vr-kB\_c4}{Probability axioms}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_3\_transcripts.pdf}{transcripts})

We have so far discussed the first step involved in the \emph{construction of a
probabilistic model}, namely, the \textbf{construction of a sample space}, which
is a description of the possible outcomes of a probabilistic experiment.

We now come to the second and much more interesting part. We need to \emph{specify which
outcomes are more likely to occur} and which ones are \emph{less likely to occur} and
so on. And we will do that by \textbf{assigning probabilities to the different outcomes}.
However, as we try to do this assignment, we run into some
kind of difficulty, which is the following.

Remember the previous experiment involving a
continuous sample space, which was the unit square and in
which we throw a dart at random and record the point
that occurred:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-017.png}}
  \end{center}
\end{figure}

In this experiment, what do you think is the probability
of a particular point?

Let's say what is the probability that my dart hits
exactly the center of this square.
Well, this probability would be essentially 0.
Hitting the center \emph{exactly with infinite
  precision} should be 0.

And so it's natural that \emph{in such a continuous model any
individual point should have a 0 probability}.
For this reason instead of assigning probabilities to
individual points, we will instead \emph{assign probabilities
  to whole sets}, that is, to subsets of the sample space.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-018.png}}
  \end{center}
\end{figure}

So here we have our sample space, which is some
abstract set omega:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-019.png}}
  \end{center}
\end{figure}

Here is a subset of the sample space, call it capital A:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-020.png}}
  \end{center}
\end{figure}

We're going to assign a probability to
that subset A, which we're going to denote with this
notation, $P(A)$, which we read as the probability of set A:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-021.png}}
  \end{center}
\end{figure}

So
\emph{probabilities will be assigned to subsets}.
And these will not cause us difficulties in the continuous
case because even though individual points would have 0
probability, if you ask me what are the odds that my dart
falls in the upper half, let's say, of this diagram, then
that should be a reasonable positive number:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-022.png}}
  \end{center}
\end{figure}

So even though individual outcomes may have 0
probabilities, sets of outcomes in general would be
expected to have positive probabilities.

So coming back, we're going to \emph{assign probabilities to the
various subsets} of the sample space.
And here comes a piece of terminology, that a subset of
the sample space is called an \textbf{event}.

Why is it called an event?
Because once we carry out the experiment and we observe the
outcome of the experiment, either this outcome is inside
the set A and in that case we say that \emph{event A has occurred},
or the outcome falls outside the set A in which case we say
that \emph{event A did not occur}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-023.png}}
  \end{center}
\end{figure}

Now we want to move on and describe certain rules.
The rules of the game in probabilistic models, which
are basically the rules that these
probabilities should satisfy.
They shouldn't be completely arbitrary.

First, by convention, probabilities are always given
in the range between 0 and 1. Intuitively:

\begin{itemize}[noitemsep]
\item probability 0 means that we believe that
  something practically cannot happen
\item probability 1 means that we're practically certain
  that an event of interest is going to happen.
\end{itemize}

So we want to specify rules of these kind for probabilities.
These \emph{rules that any probabilistic model should
satisfy} are called the \textbf{axioms of probability theory}.

Our first axiom is a \textbf{nonnegativity axiom}, $P(A) \ge 0$.
Namely, probabilities will always be
non-negative numbers.
It's a reasonable rule.

The second rule is that if the subset that we're looking at
is actually not a subset but is the \emph{entire sample space}
$\Omega$, the probability of it should always be equal to 1.
What does that mean? We know that the outcome is going to be an element of the
sample space.
This is the definition of the sample space.
So we have absolute certainty that our outcome is going to
be in $\Omega$.
Or in different language we have absolute certainty that
event $\Omega$ is going to occur.
And we capture this certainty by saying that the probability
of event omega is equal to 1: $P(\Omega )= 1 $.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-024.png}}
  \end{center}
\end{figure}

These two axioms are pretty simple and very intuitive.
The more interesting axiom is the next one that says
something a little more complicated.

Before we discuss that particular axiom, a quick
reminder about set theoretic notation.
If we have two sets, let's say a set A, and another set,
another set B, we use this particular notation, $A \cap B$, which we
read as "A intersection B" to refer to the collection of
elements that belong to both A and B. So in this picture, the
intersection of A and B is this shaded set:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-025.png}}
  \end{center}
\end{figure}

We use this notation, $A \cup B$, which we read as "A union B", to refer
to the set of elements that belong to A
or to B or to both. So in terms of this picture, the union of the two sets
would be this blue set:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-026.png}}
  \end{center}
\end{figure}

After this reminder about set theoretic notation, now let us
look at the form of the third axiom.
What does it say?
If we have two sets, \emph{two events}, \emph{two subsets} of the
sample space, which are \emph{disjoint}.
So here's our sample space.
And here are the two sets that are disjoint:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-027.png}}
  \end{center}
\end{figure}

In mathematical terms, two sets being disjoint means that
\emph{their intersection has no elements}, $A \cap B = \varnothing$.
So their intersection is the empty set.
And we use this symbol, $\varnothing$, here to denote the empty set.

So \emph{if the intersection of two sets is empty}, then the
probability that the outcome of the experiments falls in
the union of A and B, that is, the probability that the
outcome is here or there, is equal to the \emph{sum of the
probabilities of these two sets}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-028.png}}
  \end{center}
\end{figure}

This is called the \textbf{additivity} axiom.
So it says that \emph{we can add probabilities of different
sets when those two sets are disjoint}.

In some sense we can think of probability as being one pound
of some substance which is spread over our sample space
and the probability of A is how much of that substance is
sitting on top of a set A. So what this axiom is saying is
that the total amount of that substance sitting on top of A
and B is how much is sitting on top of A plus how much is
sitting on top of B. And that is the case whenever the sets
A and B are disjoint from each other.

The additivity axiom needs to be refined a bit.
We will talk about that a little later.
Other than this refinement, \emph{these three axioms are the
only requirements in order to have a
legitimate probability model}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-029.png}}
  \end{center}
\end{figure}

At this point you may ask, shouldn't there be more
requirements?
Shouldn't we, for example, say that probabilities cannot be
greater than 1?
Yes and no.
We do not want probabilities to be larger than 1, but we do
not need to say it.
As we will see in the next segment, such a requirement
follows from what we have already said.
And the same is true for several other natural
properties of probabilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercises: axioms}
\label{un1-lec1-exec-axioms}

\begin{exercise}
  \textbf{Axioms}\\
  Let $A$ and $B$ be events on the same sample space, with $P(A)=0.6$ and
  $P(B)=0.7$. Can these two events be disjoint?

  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] Yes
  \item[$\bigcirc$] No
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Simple properties of probabilities}
\label{un1-lec1-simple-properties}

Video: \href{https://www.youtube.com/watch?v=VgYkGzp\_3jk}{Simple properties of
  probabilities}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_4\_transcripts.pdf}{transcripts})

The \emph{probability axioms are the basic rules
of probability theory}. And they are surprisingly few.
But they imply many interesting properties that we
will now explore.

First we will see that what you might think of as missing
axioms are actually implied by the axioms already in place.
For example, we have an axiom that probabilities are
non-negative. We will show that probabilities are also less
than or equal to 1.
We have another axiom that says that the probability of
the entire sample space is 1.
We will show a counterpart that the probability of the
empty set is equal to 0.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-030.png}}
  \end{center}
\end{figure}

This makes perfect sense.
The empty set has no elements, so it is impossible.
There is 0 probability that the outcome of the experiment
would lie in the empty set.

We also have another intuitive property.
The probability that an event happens ($P(A)$) plus the probability
that the vendor does not happen ($P(A^c)$) exhaust all
possibilities. And these two probabilities together should add to 1
($P(A) + P(A^c) = 1$):

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-031.png}}
  \end{center}
\end{figure}

For instance, if the probability of heads is 0.6,
then the probability of tails should be 0.4.

Finally, we can generalize the additivity axiom, which was
originally given for the case of two disjoint events to the
case where we're dealing with the union of
several disjoint events.
By disjoint here we mean that the intersection of any two of
these events is the empty set ($A \cap B = \varnothing$, $A \cap C
= \varnothing$, $B \cap C = \varnothing$):

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-032.png}}
  \end{center}
\end{figure}

We will prove this for the case of three events and then
the argument generalizes for the case where we're taking
the union of k disjoint events, where k
is any finite number.
So the intuition of this result is the same as for the
case of two events.
But we will derive it formally and we will also use it to
come up with a way of calculating the probability of
a finite set by simply adding the probabilities of its
individual elements:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-033.png}}
  \end{center}
\end{figure}

All of these statements that we just
presented are intuitive.
And you do not to really need to be
convinced about their validity.
Nevertheless, it is instructive to see how these
statements follow from the axioms that
we have put in place:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-034.png}}
  \end{center}
\end{figure}

So we will now present the arguments based only on the
three axioms that we have available.
And in order to be able to refer to these axioms, let us
give them some names, call them axioms A, B, and C:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-035.png}}
  \end{center}
\end{figure}

We start as follows.
Let us look at the sample space and a subset of that
sample space.
Call it $A$. And consider the complement of that subset, $A^c$.
The complement is the set of all elements that do not
belong to the set $A$. So a set together with its complement
make up everything, which is the entire sample space: $A \cup A^c = \Omega$.
On the other hand, if an element belongs to a set A, it
does not belong to its complement.
So the intersection of a set with its complement
is the empty set, $A \cap A^c = \varnothing$:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-036.png}}
  \end{center}
\end{figure}

Now we argue as follows.
We have that the probability of the entire sample space is
equal to 1.
This is true by our second axiom, $1 = P(\Omega)$.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-037.png}}
  \end{center}
\end{figure}

Now the sample space, as we just discussed, can be written
as the union of an event and the complement of that event.
This is just a set theoretic relation:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-038.png}}
  \end{center}
\end{figure}

And next since a set and its complement are disjoint, this
means that we can apply the additivity axiom and write
this probability as the sum of the probability of event A
with the probability of the complement of A. This is one
of the relations that we had claimed and which we have now
established:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-039.png}}
  \end{center}
\end{figure}

Based on this relation, we can also write that the
probability of an event A is equal to 1 minus the
probability of the complement of that event:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-040.png}}
  \end{center}
\end{figure}

And because, by the non-negativity axiom this
quantity here, $P(A^c)$, is non-negative, 1 minus something non-negative
is less than or equal to 1. We're using here the non-negativity axiom.
And we have established another property, namely that
probabilities are always less than or equal to 1:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-041.png}}
  \end{center}
\end{figure}

Finally, let us note that 1 is the probability, always, of a
set plus the probability of a complement of that set.
And let us use this property for the case where the set of
interest is the entire sample space:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-042.png}}
  \end{center}
\end{figure}

Now, the probability of the entire sample space is itself
equal to 1, $P(\Omega)=1$.
And what is the complement of the entire sample space, $P(\Omega^c)$?
The complement of the entire sample space consists of all
elements that do not belong to the sample space.
But since the sample space is supposed to contain all
possible elements, its complement is
just the empty set.
And from this relation we get the implication that the
probability of the empty set is equal to 0:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-043.png}}
  \end{center}
\end{figure}

This establishes yet one more of the properties that we had
just claimed a little earlier.

We finally come to the proof of the generalization of our
additivity axiom from the case of two disjoint events to the
case of three disjoint events.
So we have our sample space.
And within that sample space we have three
events, three subsets.
And these subsets are disjoint in the sense that any two of
those subsets have no elements in common:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-044.png}}
  \end{center}
\end{figure}

And we're interested in the probability of the union of A,
B, and C, $P(A \cup B \cup C)$.
How do we make progress?
We have an additivity axiom in our hands, which applies to
the case of the union of two disjoint sets.
Here we have three of them.
But we can do the following trick.
We can think of the union of A, B, and C as consisting of
the union of this blue set with that green set:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-045.png}}
  \end{center}
\end{figure}

Formally, what we're doing is that we're expressing the
union of these three sets as follows.
We form one set by taking the union of A with B. And we have
the other set C. And the overall union can be thought
of as the union of these two sets:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-046.png}}
  \end{center}
\end{figure}

Now since the three sets are disjoint, this implies that
the blue set is disjoint from the green set and so we can
use the additivity axiom here to write this probability as
the probability of A union B plus the probability of C. And
now we can use the additivity axiom once more since the sets
A and B are disjoint to write the first term as probability
of A plus probability of B. We carry over the last term and
we have the relation that we wanted to prove:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-047.png}}
  \end{center}
\end{figure}

This is the proof for the case of three events.
You should be able to follow this line of proof to write an
argument for the case of four events and so on.
And you might want to continue by induction.
And eventually you should be able to prove that if the sets
$A_1,\cdots A_k$ are disjoint, then the probability of the union
of those sets is going to be equal to the sum of their
individual probabilities:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-048.png}}
  \end{center}
\end{figure}

So this is the generalization to the case where we're
dealing with the union of finitely many disjoint events.

A very useful application of this comes in the case where
we want to calculate the \emph{probability of a finite set}.
So here we have a sample space, and within that sample space we have some particular
elements $S_1$, $S_2$, up to $S_k$, k of them.
And these elements together form a finite set:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-049.png}}
  \end{center}
\end{figure}

What can we say about the probability
of this finite set?
The idea is to take this finite set that consists of k
elements and think of it as the union of several little
sets that contain one element each:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-050.png}}
  \end{center}
\end{figure}

So set theoretically what we're doing is that we're
taking this set with k elements and we write it as
the union of a set that contains just $S_1$, a set that
contains just the second element $S_2$, and so on, up to
the k-th element:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-051.png}}
  \end{center}
\end{figure}

We're assuming, of course, that these elements are all
different from each other.
So in that case, these sets, these single element sets, are
all disjoint.
So using the additivity property for a union of k
disjoint sets, we can write this as the sum of the
probabilities of the different single element sets:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-052.png}}
  \end{center}
\end{figure}

At this point, it is usual to start abusing, or rather,
simplifying notation a little bit.
\emph{Probabilities are assigned to sets}.
So here we're talking about the probability of a set that
contains a single element.
But intuitively, we can also talk as just the probability
of that particular element and use this simpler notation.
So when using the simpler notation, we will be talking
about the probabilities of individual elements.
Although in terms of formal mathematics, what we really
mean is the \textbf{probability of this event} that's \emph{comprised
only of a particular element $S_1$} and so on:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-053.png}}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Simple properties}
\label{un1-lec1-exec-simple-properties}

\begin{exercise}
  \textbf{Simple properties}\\
  Let $A$, $B$, and $C$ be disjoint subsets of the sample space. For each one of
  the following statements, determine whether it is true or false. Note: "False"
  means ``not guaranteed to be true.''

  $P(A) + P(A^c) + P(B) = P(A \cup A^c \cup B)$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] True
  \item[$\bigcirc$] False
  \end{itemize}

  $P(A) + P(B) \le 1$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] True
  \item[$\bigcirc$] False
  \end{itemize}

  $P(A^c) + P(B) \le 1$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] True
  \item[$\bigcirc$] False
  \end{itemize}

  $P(A \cup B \cup C) \ge P(A \cup B)$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] True
  \item[$\bigcirc$] False
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{More properties of probabilities}
\label{un1-lec1-more-properties}

Video: \href{https://www.youtube.com/watch?v=3gV4LWWhWwo}{More properties of
  probabilities}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_5\_transcripts.pdf}{transcripts})

We will now continue and derive some additional
properties of probability laws which are, again, consequences
of the axioms that we have introduced.

The first property is the following.
If we have two sets and one set is
inside the other, $A \subset B$, or $B \supset A$ ---
so we have a picture as follows.
We have our sample space.
And we have a certain set, A.
And then we have a certain set, B, which is even bigger.
So the set B is the bigger blue set.
So if B is a set which is larger than A, then,
naturally, the probability that the outcome falls inside
B should be at least as big as the probability that the
outcome falls inside A:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-054.png}}
  \end{center}
\end{figure}

How do we prove this formally?
The set B can be expressed as a union of two pieces.
One piece is the set A itself.
The second piece is whatever elements of B there are, that
do not belong in A. What are these elements?
They are elements that belong to B. And they do not belong
to A, which means that they belong to the complement of A.
So we have expressed the set B as the union of two pieces.
Now this piece is A. This piece here is outside A. So
these two pieces are disjoint:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-055.png}}
  \end{center}
\end{figure}

And so we can apply the additivity axiom, and write
that the probability of B is equal to the probability of A
plus the probability of the other set:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-056.png}}
  \end{center}
\end{figure}

And since probabilities are non-negative, this expression
here is at least as large as the probability of A. And this
concludes the proof of the property that
we wanted to show.
Indeed, the probability of A is less than or equal to the
probability of B:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-057.png}}
  \end{center}
\end{figure}

The next property we will show is the following.
It allows us to write the probability of the union of
two sets for the case now, where the two sets are not
necessarily disjoint:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-058.png}}
  \end{center}
\end{figure}

So the picture is as follows.
We have our two sets, A and B. These sets are
not necessarily disjoint:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-059.png}}
  \end{center}
\end{figure}

And we want to say something about the probability of the
union of A and B.
Now the union of A and B consists of three pieces.
One piece is this one here.
And that piece consists of those elements of A that do
not belong to B: $A \cap B^c$. So they belong to B complement.
This set has a certain probability, let's call it
little ``a'' and indicate it on this diagram:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-060.png}}
  \end{center}
\end{figure}

So ``a'' is the probability of this piece.

Another piece is this one here, which is the
intersection of A and B, $A \cap B$. It has a certain probability that
we denote by little ``b''.
This is the probability of A intersection B, $P(A \cap B)$.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-061.png}}
  \end{center}
\end{figure}

And finally, there's another piece, which is out here.
And that piece has a certain probability ``c''.
It is the probability of that set.
And what is that set?
That set is the following.
It's that part of B that consists of elements that do
not belong in A. So it's B intersection with the
complement of A, $B \cap A^c$:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-062.png}}
  \end{center}
\end{figure}

Now let's express the two sides of this equality here in
terms of little ``a'', little ``b'', and little ``c'', and see whether
we get the same thing.
So the probability of A union B. A union B consists of these
three pieces that have probabilities little ``a'', little
``b'', and little ``c'', respectively.
And by the additivity axiom, the probability of the union
of A and B is the sum of the probabilities
of these three pieces:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-063.png}}
  \end{center}
\end{figure}

Let's look now at the right hand side of that equation and
see whether we get the same thing.
The probability of A plus the probability of B, minus the
probability of A intersection B is equal to the following:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-064.png}}
  \end{center}
\end{figure}

$A$ consists of two pieces that have probabilities little ``a''
and little ``b'':

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-065.png}}
  \end{center}
\end{figure}

The set B consists of two pieces that have probabilities
little ``b'' and little ``c'':

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-066.png}}
  \end{center}
\end{figure}

And then we subtract the probability of the
intersection, which is ``b'':

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-067.png}}
  \end{center}
\end{figure}

And we notice that we can cancel here one ``b''
with another ``b''. And what we are left with is ``a'' plus ``b'' plus ``c'':

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-068.png}}
  \end{center}
\end{figure}

So this checks.
And indeed we have this equality here.
We have verified that it is true.

One particular consequence of the equality that we derived
is the following.
Since this term here is always non-negative,

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-069.png}}
  \end{center}
\end{figure}

this means that the $P(A \cup B)$ is \emph{always} less than or equal
to the $P(A) + P(B)$:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-070.png}}
  \end{center}
\end{figure}

This
inequality here is quite useful whenever we want to
argue that a certain probability is
smaller than something.
And it has a name.
It's called the \textbf{union bound}.

We finally consider one last consequence of our axioms.
And namely, we are going to derive an expression, a way of
calculating the probability of the union of three sets, not
necessarily disjoint:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-071.png}}
  \end{center}
\end{figure}

So we have our sample space.
And within the sample space there are three sets --- set A,
set B, and set C:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-072.png}}
  \end{center}
\end{figure}

We are going to use a set theoretic relation.
We are going to express the union of these three sets as
the union of three disjoint pieces:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-073.png}}
  \end{center}
\end{figure}

What are these disjoint pieces?
One piece is the set A itself:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-074.png}}
  \end{center}
\end{figure}

The second piece is going to be that part of B which is
outside A. So this is the intersection of B with the
complement of A:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-075.png}}
  \end{center}
\end{figure}

The third piece is going to be whatever is left in order to
form the union of the three sets.
What is left is that part of C that does not belong to A and
that does not belong to B. So that part is C intersection
with A complement and B complement:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-076.png}}
  \end{center}
\end{figure}

Now this set here, of course, is the same as that set
because intersection of two sets is the same no matter in
which order we take the two sets.
And similarly, the set that we have here is the same one that
appears in that expression:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-077.png}}
  \end{center}
\end{figure}

Now we notice that these three pieces, the red, the blue, and
the green, are disjoint from each other.
So by the additivity axiom, the probability of this union
here is going to be the sum of the probabilities
of the three pieces.
And that's exactly the expression
the we have up here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: More properties}
\label{un1-lec1-exec-more-properties}

\begin{exercise}
  \textbf{More properties}\\
  Let $A$, $B$, and $C$ be subsets of the sample space, not necessarily disjoint.
  For each one of the following statements, determine whether it is true or false.
  Note: ``False'' means ``not guaranteed to be true.''

  $P\left[(A \cap B) \cup (C \cap A^c)\right] \le P(A \cup B \cup C)$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] True
  \item[$\bigcirc$] False
  \end{itemize}

  $P(A \cup B \cup C) = P(A \cap C^c) + P(C) + P(B \cap A^c \cap C^c)$
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] True
  \item[$\bigcirc$] False
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{A discrete example}
\label{un1-lec1-discrete-example}

Video: \href{https://www.youtube.com/watch?v=MqHBUGcvpGI}{A discrete example}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_6\_transcripts.pdf}{transcripts})

Let us now move from the abstract to the concrete.
Recall the example that we discussed earlier where we
have two rolls of a tetrahedral die.
So there are 16 possible outcomes
illustrated in this diagram:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-078.png}}
  \end{center}
\end{figure}

To continue, now we \emph{need to specify a probability law},
some kind of probability assignment.
To keep things simple, we're going to make the assumption
that the 16 possible outcomes are all equally likely.
And each outcome has a probability of 1 over 16 ($1/16$).
Given this assumption, we will now proceed to calculate
certain probabilities.

Let us look first at the probability that X, which
stands the result of the first roll, is equal to 1: $X=1$.
The way to calculate this probability is to identify
what exactly that event is in our picture of the sample
space, and then calculate.
The event that X is equal to 1 can happen in four different
ways that correspond to these four particular outcomes:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-079.png}}
  \end{center}
\end{figure}

Each one of these outcomes has a probability of 1 over 16.
The probability of this event is the sum of the
probabilities of the outcomes that it contains.
So it is 4 times 1/16, equal to 1/4:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-080.png}}
  \end{center}
\end{figure}

Let now $Z$ stand for the smaller of the two numbers
that came up in our two rolls.
So for example, if $X = 2$ and $Y = 3$, then $Z = 2$,
which is the smaller of the two:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-081.png}}
  \end{center}
\end{figure}

Let us try to calculate the probability that the smaller
of the two outcomes is equal to 4, $Z = 4$.
Now for the smaller of the two outcomes to be equal to 4, we
must have that both X and Y are equal to 4.
So this outcome here (in blue) is the only way that this particular
event can happen:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-082.png}}
  \end{center}
\end{figure}

Since there's only one outcome that makes the event happen,
the probability of this event is the probability of that
outcome and is equal to 1/16.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-083.png}}
  \end{center}
\end{figure}

For another example, let's calculate the probability that
the minimum is equal to 2.
What does it mean that the minimum is equal to 2?
It means that one of the dice resulted in a 2, and the other
die resulted in a number that's 2 or larger.
So we could have both equal to 2.
We could have X equal to 2, but Y larger.
Or we could have Y equal to 2 and X something larger.
This green event, this green set, is the set of all
outcomes for which the minimum of the two
rolls is equal to 2:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-084.png}}
  \end{center}
\end{figure}

There's a total of five such outcomes.
Each one of them has probably 1 over 16.
And we have discussed that for finite sets, \emph{the probability
of a finite set is the sum of the probabilities of the
elements of that set}.
So we have five elements here, each one with
probability 1 over 16, and get 5 over 16, and this is the answer to this problem:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-085.png}}
  \end{center}
\end{figure}

This particular example that we saw here is a special case
of what is called a \textbf{discrete uniform law}.
In a discrete uniform law, we have a \emph{finite sample
space}. And it has n elements.
And we assume that these n elements are \emph{equally likely}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-086.png}}
  \end{center}
\end{figure}

Now since the probability of omega, the probability of the
entire sample space, is equal to 1, this means that each one
of these elements must have probability 1/n.
That's the only way that the sum of the probabilities of
the different outcomes would be equal to 1 as required by
the normalization axiom.

Consider now some subset of the sample space, an event A
that, exactly k elements.
What is the probability of the set A?
It's the \emph{sum of the probabilities of its elements}.
There are k elements.
And each one of them has a probability of 1/n.
And this way we can find the probability of the set A:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-087.png}}
  \end{center}
\end{figure}

So when we have a \emph{discrete uniform probability law}, we
can calculate probabilities by simply \emph{counting the number of
elements of omega}, which is n, finding the number n, and
\emph{counting the number of elements of the set A}. That's
the reason why counting will turn out to be
an important skill.
And there will be a whole lecture devoted to this
particular topic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Discrete probability calculations}
\label{un1-lec1-exec-discrete-probability}

\begin{exercise}
  \textbf{Discrete probability calculations}\\
  Consider the same model of two rolls of a tetrahedral die, with all 16 outcomes
  equally likely. Find the probability of the following events:

  a) The value in the first roll is strictly larger than the value in the second roll.
  
  b) The sum of the values obtained in the two rolls is an even number.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{A continuous example}
\label{un1-lec1-continuous-example}

Video: \href{https://www.youtube.com/watch?v=sT3xw1TZ7oU}{A continuous example}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_7\_transcripts.pdf}{transcripts})

We will now go through a probability calculation for
the case where we have a \emph{continuous sample space}.
We revisit our earlier example in which we were throwing a
dart into a square target, the square target
being the unit square.
And we were guaranteed that our dart would fall somewhere
inside this set.

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-088.png}}
  \end{center}
\end{figure}

So our sample space is the unit square itself.
We have a description of the sample space, but we do not
yet have a probability law.
We need to specify one.

The choice of a probability law could be arbitrary.
\emph{It's up to us to choose how to model a certain situation}.
And to keep things simple, we're going to assume that our
probability law is a uniform one, which means that the
probability of any particular subset of the sample space is
going to be the area of that subset.
So if we have some subset lying somewhere here and we
ask what is the probability that we fall into that subset?
The probability is exactly the area of
that particular subset:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-089.png}}
  \end{center}
\end{figure}

Once more, this is an arbitrary choice of a
probability law.
There's nothing in our assumptions so far that would
force us to make this particular choice.
And we just use it for the purposes of this example.

So now let us calculate some probabilities.
Let us look at this event.
This is the event that the sum of the two numbers that we get
in our experiment is less than or equal to 1/2:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-090.png}}
  \end{center}
\end{figure}

It is always useful to work in terms of a picture and to
depict that event in a picture of the sample space.
So in terms of that sample space, the points that make
this event to be true are just a triangle that lies below the
line, where this is the line, that's x plus y equals 1/2:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-091.png}}
  \end{center}
\end{figure}

Anything below that line, these are the outcomes that
make this event happen.
So we're trying to find the probability of this red event.
We have assumed that probability is equal to area.
Therefore, the probability we're trying to calculate is
the area of a triangle.
And the area of a triangle is \emph{1/2} times the \emph{base} of the
triangle, which is 1/2 in our case, times the \emph{height} of the
triangle, which is again 1/2 in our case.
And the end result is 1/8:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-092.png}}
  \end{center}
\end{figure}

Let us now calculate another probability.
Now, this is an event that consists of
only a single element.
We take the point 0.5, 0.3, which sits somewhere here:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-093.png}}
  \end{center}
\end{figure}

The event of interest is a set, but that set consists of
a single point.
So we're asking for the probability that our dart
falls exactly on top of that point.
What is it?
Well, it is the area of a set that
consists of a single point.
What is the area of a single point?
It is 0:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-094.png}}
  \end{center}
\end{figure}

And similarly for any other single point inside that
sample space that we might have considered, the answer is
going to be 0.

Let us now abstract from this example, as well as the
previous one, and note the following.
\textbf{Probability calculations involve a
  sequence of four steps}:

\begin{enumerate}
\item \emph{Specify the sample space}: starting with a word description of a problem, of a
  probabilistic experiment, we first write
  down the sample space.
\item \emph{Specify the probability law}: Then we specify a probability law.
  Let me emphasize again here that this step has some
  arbitrariness in it. You can choose any probability law you like, although for
  your results to be useful it would be good if your
  probability law captures the real-world phenomenon you're
  trying to model.
\item \emph{Identify an event of interest}: Typically you're interested in calculating the probability of
  some event. That event may be described in some loose manner, so you need
  to describe it mathematically. And if possible, it's always good to describe it in terms
  of a picture. Pictures are immensely useful when going through this process.
\item \emph{Calculate}: And finally, the last step is to go ahead and calculate the
  probability of the event of interest.
\end{enumerate}

Now, a probability law in principle specifies the
probability of every event, and there's
nothing else to do.
But quite often the probability law will be given
in some \emph{implicit manner}, for example, by specifying the
probabilities of only some of the events.

In that case, you may have to do some additional work to
find the probability of the particular event
that you care about.
This last step sometimes will be easy.
Sometimes it may be complicated.
But in either case, by following this four-step
procedure and by being systematic you will always be
able to come up with a single correct answer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Continuous probability calculations}
\label{un1-lec1-exec-continuous-probability}

\begin{exercise}
  \textbf{Continuous probability calculations}\\
  Consider a sample space that is the rectangular region $[0, 1] \times [0, 2]$,
  i.e., the set of all pairs $(x, y)$ that satisfy $0 \le x \le 1$ and $0 \le y \le 2$.
  Consider a “uniform" probability law, under which the probability of an event is
  \emph{half of the area of the event}. Find the probability of the following events:

  a) The two components $x$ and $y$ have the same values.
  
  b) The value, $x$ , of the first component is larger than or equal to the value, $y$,
  of the second component.

  c) The value of $x^2$ is larger than or equal to the value of $y$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Countable additivity}
\label{un1-lec1-coutable-additivity}

Video: \href{https://www.youtube.com/watch?v=oGePTQ6hFFI}{Countable additivity}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_8\_transcripts.pdf}{transcripts})

We have seen so far an example of a probability law on a
\emph{discrete and finite sample space} as well as an example
with an \emph{infinite and continuous} sample space.

Let us now look at an example involving a \emph{discrete but
infinite sample space}.
We carry out an experiment whose outcome is an arbitrary
positive integer.
As an example of such an experiment, suppose that we
keep tossing a coin and the outcome is the number of
tosses until we observe heads for the first time:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-095.png}}
  \end{center}
\end{figure}

The first heads might appear in the first toss or the
second or the third, and so on.
So in this example, any positive integer is possible.
And so our \emph{sample space is infinite}.

Let us now specify a probability law.
A \emph{probability law should determine the probability of
every event, of every subset of the sample space}.
That is, the probability of every
set of positive integers.
But instead I will just tell you the probability of events
that contain a single element.
I'm going to tell you that there is probability 1 over 2
to the n that the outcome is equal to n:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-096.png}}
  \end{center}
\end{figure}

Is this good enough?
Is this information enough to \emph{determine the probability of
any subset}?

Before we look into that question, let us first do a
quick sanity check to see whether these numbers that we
are given look like legitimate probabilities.

\emph{Do they add to 1}?
Let's do a quick check.
So the sum over all the possible values of n of the
probabilities that we're given, which is an infinite
sum starting from 1, all the way up to infinity, of 1 over
2 to the n, is equal to the following:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-097.png}}
  \end{center}
\end{figure}

First we take out a factor of 1/2 from all of these terms,
which reduces the exponent from n to n minus 1.
This is the same as running the sum from n equals 0 to
infinity of 1/2 and to the n:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-098.png}}
  \end{center}
\end{figure}

And now we have a usual infinite geometric series and
we have a formula for this.
The geometric series has a value of 1 over 1 minus the
number whose power we're taking, which is 1/2.
And after we do the arithmetic, this turns out to
be equal to 1:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-099.png}}
  \end{center}
\end{figure}

So indeed, it appears that we have the basic elements of
what it would take to have a legitimate probability law.

But now let us look into how we might calculate the
probability of some general event.
For example, the probability that the outcome is even:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-100.png}}
  \end{center}
\end{figure}

We proceed as follows.
The probability that the outcome is even, this is the
probability of an infinite set that consists of
all the even integers:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-101.png}}
  \end{center}
\end{figure}

We can write this set as the union of lots of little sets
that contain a single element each.
So it's the set containing the number 2, the set containing
the number 4, the set containing the
number 6, and so on:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-102.png}}
  \end{center}
\end{figure}

At this point we notice that we're talking about the
probability of a union of sets and these sets are disjoint
because they contain different elements.
So we can use an additivity property and say that this is
the probability of obtaining a 2, plus the probability of
obtaining a 4, plus the probability of
obtaining a 6 and so on:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-103.png}}
  \end{center}
\end{figure}

If you're curious about doing this calculation and actually
obtaining a numerical answer, you would proceed as follows.
You notice that this is 1 over 2 to the second power plus 1
over 2 to the fourth power plus 1 over 2 to the sixth
power and so on:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-104.png}}
  \end{center}
\end{figure}

Now you factor out a factor of 1/4 and what you're left is 1
plus 1 over 2 to the second power, which is 1/4, plus 1
over 2 to the fourth power, which is the same as 1/4 to
the second power and so on.
And now we have 1/4 times the infinite sum of a geometric
series, which gives us 1 over 1 minus 1/4.
And after you do the algebra you obtain a numerical answer,
which is equal to 1/3:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-105.png}}
  \end{center}
\end{figure}

But leaving the details of the calculation aside, the more
important question I want to address is the following.
Is this calculation correct?
We seem to have used an additivity
property at this point.
But the additivity properties that we have in our hands at
this point only talk about \emph{disjoint unions of finitely
  many subsets}.

Our initial axiom talked about a disjoint union of two
subsets and then later on we established a similar property
for a \emph{disjoint union of finitely many subsets}.
But here we're talking about the \emph{union of
infinitely many subsets}.
So this step here is not really allowed by what we have
in our hands:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-106.png}}
  \end{center}
\end{figure}

On the other hand, we would like our theory to allow this
kind of calculation.
The way out of this dilemma is to introduce an additional
axiom that will indeed allow this kind of calculation.
The axiom that we introduce is the following:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-107.png}}
  \end{center}
\end{figure}

If we have an \emph{infinite sequence of disjoint events},
as for example in this picture.
We have our sample space.
We have a first event, A1.
We have a second event, A2.
The third event, A3.
And so we keep continuing and we have an infinite sequence
of such events:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-108.png}}
  \end{center}
\end{figure}

Then \emph{the probability of the union of these events, of
these infinitely many events, is the sum of their individual
probabilities}.
The key word here is the word \textbf{sequence}.
Namely, these events, these sets that we're dealing with,
can be arranged so that we can talk about the first event,
A1, the second event, A2, the third one, A3, and so on:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-109.png}}
  \end{center}
\end{figure}

To appreciate the issue that arises here and to see why the
word sequence is so important, let us consider the following
calculation.
Our sample space is the unit square.
And we consider a model where the probability of a set is
its area, as in the examples that we considered earlier.

Let us now look at the probability of the overall
sample space.
Our sample space is the unit square and the unit square can
be thought of as the union of various sets that consist of
single points.
So it's the union of subsets with one element each.
And it's a union taken over all the
points in the unit square:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-110.png}}
  \end{center}
\end{figure}

Then we think about additivity.
We observe that these subsets are disjoint.
If we're considering different points, then we get disjoint
single element sets.
And then an additivity property would tells us that
the probability of this union is the sum of the
probabilities of the different single element subsets:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-111.png}}
  \end{center}
\end{figure}

Now, as we discussed before, single element subsets have 0
probability.
So we have a sum of lots of 0s and the sum of 0s should be
equal to 0:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-112.png}}
  \end{center}
\end{figure}

On the other hand, by the probability axioms, the
probability of the entire sample space
should be equal to 1.
And so we have established that 1 is equal to 0:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-113.png}}
  \end{center}
\end{figure}

This looks like a paradox.
Is it?
The catch is that there is nothing in the axioms we have
introduced so far or the properties we have established
that would justify this step:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-114.png}}
  \end{center}
\end{figure}

So this step here is questionable.
You might argue that the unit square is the union of
disjoint single element sets, which is the case that we have
in additivity axioms.
But the \emph{additivity axiom only applies when we have a
sequence of events}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-115.png}}
  \end{center}
\end{figure}

And this is not what we have here.
This is not a union of a sequence of
single element sets.

In fact, there is no way that the elements of the unit
square can be arranged in a sequence.
The unit square is said to be an \textbf{uncountable set}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-116.png}}
  \end{center}
\end{figure}

This is a deep and fundamental mathematical fact.
What it essentially says is that \emph{there are two kinds of
  infinite sets}:

\begin{itemize}
\item \emph{Countable}: Discrete ones or in formal terminology countable.
  These are sets whose elements can be arranged in a sequence,
  like the integers.
\item \emph{Uncontable}: And also uncountable sets, such as the unit square or the
  real line, whose elements cannot be arranged in a sequence.
\end{itemize}

If you're curious, you can find the proof of this
important fact in the supplementary materials that
we are providing.

After all these discussion, you may now have legitimate
suspicions about the models we have been looking at.
Is area a legitimate probability law?
Does it even satisfy countable additivity?
This question takes us into deep waters and has to do with
a deep subfield of mathematics called \emph{Measure Theory}.

Fortunately, it turns out that all is well.
Area is a legitimate probability law.
It does indeed satisfy the countable additivity axiom as
long as we only deal with ``nice subsets'' of the unit square.
Fortunately, the subsets that arise in whatever we do in
this course will be ``nice'':

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-117.png}}
  \end{center}
\end{figure}

Subsets that are not nice are quite pathological and we will
not encounter them.

At this stage we are not in a position to say anything more
that would be meaningful about these issues because they're
quite complicated and mathematically deep.
We can only say that there are some serious mathematical
subtleties.
But fortunately, they can all be overcome
in a rigorous manner.
And for the rest of this class, you can just forget
about these subtle issues.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Using countable additivity}
\label{un1-lec1-exec-using-countable-additivity}

\begin{exercise}
  \textbf{Using countable additivity}\\
  Let the sample space be the set of positive integers and suppose that $P(n)=1/2^n$,
  for $n=1,2,\ldots, n$. Find the probability of the set ${3, 6, 9, \ldots}$, that is,
  of the set of of positive integers that are multiples of 3.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: Uniform probabilities on the integers}
\label{un1-lec1-exec-unif-prob-integers}

\begin{exercise}
  \textbf{Uniform probabilities on the integers}\\
  Let the sample space be the set of positive integers. Is it possible to have a
  “uniform" probability law, that is, a probability law that assigns the same
  probability to each positive integer?

  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] Yes
  \item[$\bigcirc$] No
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exercise: On countable additivity}
\label{un1-lec1-exec-cont-additivity}

\begin{exercise}
  \textbf{On countable additivity}\\
  Let the sample space be the two-dimensional plane. For any real number $x$,
  let $A_x$ be the subset of the plane that consists of all points of the vertical
  line through the point $(x, 0)$, i.e., $A_x = {(x, y) : y \in Re}$.

  a) Do the axioms of probability theory imply that the probability of the union
  of the sets $A_x$ (which is the whole plane) is equal to the sum of the
  probabilities $P(A_x)$?
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] Yes
  \item[$\bigcirc$] No
  \end{itemize}

  b) Do the axioms of probability theory imply that
  $\displaystyle P(A_1 \cup A_2 \cup \cdots) = \sum_{x=1}^{\infty} P(A_x)$? (In
  other words, we consider only those lines for which the $x$ coordinate is
  a positive integer.)
  \begin{itemize}[noitemsep]
  \item[$\bigcirc$] Yes
  \item[$\bigcirc$] No
  \end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Interpretations and uses of probabilities}
\label{un1-lec1-interpretation-and-use}

Video: \href{https://www.youtube.com/watch?v=XsUNKLd7N5Y}{Interpretations and uses of probabilities}
(\href{Unit-1/01\_lecture\_1\_probability\_models\_and\_axioms/l01\_9\_transcripts.pdf}{transcripts})

We end this lecture sequence by stepping back to discuss
\emph{what probability theory really is} and \emph{what exactly is the
  meaning of the word probability}.

In the most narrow view, probability theory is just a
branch of mathematics.
We start with some axioms.
We consider models that satisfy these axioms, and we
establish some consequences, which are the
theorems of this theory:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-118.png}}
  \end{center}
\end{figure}

You could do all that without ever asking the question of
what the word "probability" really means.
Yet, one of the theorems of probability theory, that we
will see later in this class, is that probabilities can be
interpreted as frequencies, very loosely speaking:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-119.png}}
  \end{center}
\end{figure}

If I have a fair coin, and I toss it infinitely many times,
then the fraction of heads that I will
observe will be one half.
In this sense, the probability of an event, A, can be
interpreted as the \emph{frequency with which event A will occur
in an infinite number of repetitions of the experiment}.
But is this all there is?
If we're dealing with coin tosses, it makes sense to
think of probabilities as frequencies:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-120.png}}
  \end{center}
\end{figure}

But consider a statement such as the "current president of
my country will be reelected in the next election with
probability 0.7".
It's hard to think of this number, 0.7, as a frequency.
It does not make sense to think of infinitely many
repetitions of the next election.

In cases like this, and in many others, it is better to
think of \emph{probabilities as just some way of
  describing our beliefs}:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-121.png}}
  \end{center}
\end{figure}

And if you're a betting person, probabilities can be
thought of as some numerical guidance into what kinds of
bets you might be willing to make.

But now if we think of probabilities as beliefs, you
can run into the argument that, well, beliefs are
subjective.
Isn't probability theory supposed to be an objective
part of math and science?
Is probability theory just an exercise in subjectivity?
Well, not quite.
There's more to it:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-122.png}}
  \end{center}
\end{figure}

Probability, at the minimum, gives us some \emph{rules for
thinking systematically about uncertain situations}.
And if it happens that \emph{our probability model, our
subjective beliefs}, have some relation with the real world,
then probability theory can be a very useful tool for making
predictions and decisions that apply to the real world.

Now, whether your predictions and decisions will be any good
will depend on whether you have chosen a good model.
Have you chosen a model that's provides a good enough
representation of the real world?
How do you make sure that this is the case?

There's a whole field, the field of \emph{statistics, whose
purpose is to complement probability theory by using
data to come up with good models}.

And so we have the following diagram that summarizes the
relation between the real world, statistics, and
probability:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/l01-123.png}}
  \end{center}
\end{figure}

The real world generates data.
The field of statistics and inference uses these data to
come up with probabilistic models.
Once we have a probabilistic model, we use probability
theory and the analysis tools that it provides to us.
And the results that we get from this analysis lead to
predictions and decisions about the real world.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mathematical background: Sets; sequences, limits, and series;
  (un)countable sets}
\label{un1-math-back}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Mathematical background overview}
\label{un1-math-back-ovw}

This collection of clips reviews some background material about sets, including
De Morgan's laws (see also Section 1.1 of the text), sequences and their
convergence, infinite series, infinite series with multiple indices, and
uncountable sets.

Video: \href{https://www.youtube.com/watch?v=q2cHGd\_MP\_4}{Mathematical background: Overview}
(\href{Unit-1/02\_mathematical\_background/mb\_1\_transcripts.pdf}{transcripts},
\href{Unit-1/02\_mathematical\_background/mb\_overview\_annotated\_slides.pdf}{annotated slides})

In this sequence of segments, we review some mathematical
background that will be useful at various
places in this course.
Most of what is covered, with the exception of the last
segment, is material that you may have seen before.
But this could still be an opportunity to refresh some of
these concepts:

\begin{figure}[H]
  \begin{center}
    %\caption{}
    %\label{fig:}
    \fbox{\includegraphics[scale=0.6]{imagens/unit-1/mb-001.png}}
  \end{center}
\end{figure}

I should say that this is intended
to be just a refresher.
Our coverage is not going to be complete in any sense.

What we will talk about is sets, various definitions
related to sets, and some basic properties, including De
Morgan's laws.

We will talk about what a sequence is and what it means
for a sequence to converge to something.

We will talk about infinite series.
And as an example, we will look at the geometric series.

Then we will talk about some subtleties that arise when you
have sums of terms that are indexed with multiple indices.

And finally, probably the most sophisticated part, will be a
discussion of countable versus uncountable sets.

Countable sets are like the integers.
Uncountable sets are like the real line.
And they're fundamentally different.
And this fundamental difference reflects itself
into fundamentally different probabilistic models ---
models that involve discrete experiments and outcomes
versus models that involve continuous outcomes.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TERMINA O DOCUMENTO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}









